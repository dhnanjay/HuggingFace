{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN3ZFxHx9h5mKXvJRZc3NVR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ced557bc06754653863e91fa447b55d4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ad9d251a066048cda8637ca55cb9a93f","IPY_MODEL_95979b3dd1774ab18d319790d268da5c","IPY_MODEL_60400048faab49978cc17a83498b91cb"],"layout":"IPY_MODEL_f94922a84a66404e8d2f3781e7702739"}},"ad9d251a066048cda8637ca55cb9a93f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd784ec36985474e8fabf73008779134","placeholder":"​","style":"IPY_MODEL_2c2b630539544ef88c211b6cf0d71f15","value":"Downloading (…)okenizer_config.json: 100%"}},"95979b3dd1774ab18d319790d268da5c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3ed2fea40274606bfd20ecd9b7a1f29","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6705d16b705b427f828a56585d7943d9","value":48}},"60400048faab49978cc17a83498b91cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c89811caef04190a6f378703915896c","placeholder":"​","style":"IPY_MODEL_2802fd919a244c45b708efa2548ac451","value":" 48.0/48.0 [00:00&lt;00:00, 854B/s]"}},"f94922a84a66404e8d2f3781e7702739":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd784ec36985474e8fabf73008779134":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c2b630539544ef88c211b6cf0d71f15":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a3ed2fea40274606bfd20ecd9b7a1f29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6705d16b705b427f828a56585d7943d9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4c89811caef04190a6f378703915896c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2802fd919a244c45b708efa2548ac451":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"32cc2490dd4c4efc9a844951a3f582ef":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_741a7662fe924e9bb497238fb6dc8504","IPY_MODEL_8a3b73e2cb6b475197335339d7581c1b","IPY_MODEL_24066f5755d54b7da6a0fd356b8a24ad"],"layout":"IPY_MODEL_82a3b79a3baa46ce80831c0a25511917"}},"741a7662fe924e9bb497238fb6dc8504":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ff63995b859497aa0ad07f572cbe095","placeholder":"​","style":"IPY_MODEL_0abd05f2479c413e8825d393aa7a4eff","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"8a3b73e2cb6b475197335339d7581c1b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7255a9abb8314b7a90bfb59a9f4cb187","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5b7cea21398747d0a9f6cfb28a8d194f","value":231508}},"24066f5755d54b7da6a0fd356b8a24ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_71296cd55f88408998fe413858062afc","placeholder":"​","style":"IPY_MODEL_38c4cdd6a88245a4a169d4a7766b8592","value":" 232k/232k [00:00&lt;00:00, 6.50MB/s]"}},"82a3b79a3baa46ce80831c0a25511917":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ff63995b859497aa0ad07f572cbe095":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0abd05f2479c413e8825d393aa7a4eff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7255a9abb8314b7a90bfb59a9f4cb187":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b7cea21398747d0a9f6cfb28a8d194f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"71296cd55f88408998fe413858062afc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38c4cdd6a88245a4a169d4a7766b8592":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["<h2> Behind the pipeline </h2>\n","\n","**Reference Material**\n","\n","\n","[HF Video](https://https://youtu.be/1pedAIvTWXk)\n","\n","[HF Course](https://huggingface.co/course/chapter2/2?fw=pt)\n","\n","[Embedding vs Token](https://vaclavkosar.com/ml/transformer-embeddings-and-tokenization)\n","\n"],"metadata":{"id":"3wIjU_Nc_rKU"}},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"83yKTYKiWBvI","executionInfo":{"status":"ok","timestamp":1678912658796,"user_tz":360,"elapsed":1587,"user":{"displayName":"Dhananjay Kumar","userId":"03056671537097392811"}},"outputId":"b9493c17-2fe6-48b7-e46c-d7a7290b046b"},"outputs":[{"output_type":"stream","name":"stdout","text":["These are tokens! tensor([[ 101, 2023, 2003, 1037, 7953, 1012,  102]])\n","This are decoded tokens! [CLS]\n","This are decoded tokens! this\n","This are decoded tokens! is\n","This are decoded tokens! a\n","This are decoded tokens! input\n","This are decoded tokens! .\n","This are decoded tokens! [SEP]\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["#-=======================-#\n","tensor([[[ 0.0390, -0.0123, -0.0208,  ...,  0.0607,  0.0230,  0.0238],\n","         [-0.0558,  0.0151,  0.0031,  ..., -0.0140, -0.0277,  0.0139],\n","         [-0.0440, -0.0236, -0.0283,  ...,  0.0053, -0.0081,  0.0170],\n","         ...,\n","         [-0.0788,  0.0202, -0.0352,  ...,  0.0119, -0.0037, -0.0402],\n","         [-0.0244, -0.0138, -0.0078,  ...,  0.0069,  0.0057, -0.0016],\n","         [-0.0199, -0.0095, -0.0099,  ..., -0.0235,  0.0071, -0.0071]]],\n","       grad_fn=<EmbeddingBackward0>)\n"]}],"source":["# pip install transformers && pip install torch\n","# Source : https://vaclavkosar.com/ml/transformer-embeddings-and-tokenization\n","\n","from transformers import DistilBertTokenizerFast, DistilBertModel\n","\n","tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n","tokens = tokenizer.encode('This is a input.', return_tensors='pt')\n","print(\"These are tokens!\", tokens)\n","for token in tokens[0]:\n","    print(\"This are decoded tokens!\", tokenizer.decode([token]))\n","\n","model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n","print(\"#-=======================-#\")\n","print(model.embeddings.word_embeddings(tokens))\n","# for e in model.embeddings.word_embeddings(tokens)[0]:\n","#     print(\"This is an embedding!\", e)\n"]},{"cell_type":"code","source":["len(model.embeddings.word_embeddings(tokens)[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qzX1qFtYW5sr","executionInfo":{"status":"ok","timestamp":1678902014350,"user_tz":360,"elapsed":130,"user":{"displayName":"Dhananjay Kumar","userId":"03056671537097392811"}},"outputId":"c36119f3-64aa-4b09-e956-2bf3afd4884a"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# !pip install transformers\n","# !pip install torch"],"metadata":{"id":"junZ0JqHWGJq","executionInfo":{"status":"ok","timestamp":1678902514184,"user_tz":360,"elapsed":155,"user":{"displayName":"Dhananjay Kumar","userId":"03056671537097392811"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# https://huggingface.co/course/chapter2/2?fw=pt\n","\n","from transformers import AutoTokenizer\n","checkpoint  = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","raw_inputs = ['This is a input.', \"This is not\"]\n","inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")"],"metadata":{"id":"zAbenVa7WJgI","executionInfo":{"status":"ok","timestamp":1678902905252,"user_tz":360,"elapsed":191,"user":{"displayName":"Dhananjay Kumar","userId":"03056671537097392811"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["\n","inputs "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3pdpsAnqWTiw","executionInfo":{"status":"ok","timestamp":1678902906215,"user_tz":360,"elapsed":123,"user":{"displayName":"Dhananjay Kumar","userId":"03056671537097392811"}},"outputId":"3274f73a-6dd4-43a8-84b9-5cc94da04be5"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[ 101, 2023, 2003, 1037, 7953, 1012,  102],\n","        [ 101, 2023, 2003, 2025,  102,    0,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 0, 0]])}"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["In the above output of tokens from the Transformer AutoTokenizer, the 'attention_mask' signifies which tokens are actual words and which ones are padding tokens.\n","\n","In the example you provided, the 'input_ids' tensor contains the tokenized input sequence, which consists of seven tokens. \n","\n","The first token [101] is the special token [CLS], which is added to the beginning of the sequence. The fourth token [1037] corresponds to the word 'a', and the fifth token [7953] corresponds to the word 'sentence'.\n","\n","The second and third tokens [2023, 2003] correspond to the words 'is' and 'this', respectively, and the sixth and seventh tokens [1012, 102] correspond to the special tokens[SEP] and [PAD], respectively.\n","\n","The 'attention_mask' tensor is a binary tensor that has the same shape as the 'input_ids' tensor. It has a value of 1 for every actual word token in the input sequence and a value of 0 for every padding token. \n","In this example, since all the tokens are actual words, the 'attention_mask' tensor has a value of 1 for every element. This helps the Transformer model to distinguish between the actual words and the padding tokens during training and inference."],"metadata":{"id":"ktKJzaIb-qIQ"}},{"cell_type":"code","source":["# Tokens from the previous output for comparison\n","tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"erfZRmIgZdph","executionInfo":{"status":"ok","timestamp":1678902708950,"user_tz":360,"elapsed":129,"user":{"displayName":"Dhananjay Kumar","userId":"03056671537097392811"}},"outputId":"e593d781-073e-4774-de8b-5fbd1303f083"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 101, 2023, 2003, 1037, 7953, 1012,  102]])"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["The purpose of the below code is to encode a tokenized input sequence using a pre-trained Transformer model and obtain a contextualized representation of the sequence.\n","\n","More specifically, the code uses the AutoModel class from the Hugging Face Transformers library to load a pre-trained Transformer model specified by the checkpoint string. The model(**inputs) line applies the pre-trained model to the input sequence, which is represented as a dictionary of tokenized tensors in the inputs variable.\n","\n","The outputs variable is a dictionary that contains the encoded representation of the input sequence in various forms. One of these representations is the last_hidden_state tensor, which is a 3-dimensional tensor that contains the contextualized embeddings of the input tokens.\n","\n","The size of the last_hidden_state tensor is (batch_size, sequence_length, hidden_size), where batch_size is the number of input sequences in the batch, sequence_length is the length of the longest input sequence in the batch, and hidden_size is the size of the hidden layer in the pre-trained Transformer model. The elements of the last_hidden_state tensor are the embeddings of the input tokens after passing through the Transformer layers.\n","\n","So to summarize, the purpose of the code is to encode a tokenized input sequence using a pre-trained Transformer model and obtain a tensor of embeddings for the input tokens, which capture both their semantic and syntactic properties in the context of the sequence."],"metadata":{"id":"rYI0i7VM4Zoe"}},{"cell_type":"code","source":["from transformers import AutoModel\n","model = AutoModel.from_pretrained(checkpoint)\n","outputs = model(**inputs)\n","print(outputs.last_hidden_state.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VnFDQxDDZkjR","executionInfo":{"status":"ok","timestamp":1678903339391,"user_tz":360,"elapsed":734,"user":{"displayName":"Dhananjay Kumar","userId":"03056671537097392811"}},"outputId":"c62ca1bd-7f21-4f40-fc80-87fd3146e925"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([2, 7, 768])\n"]}]},{"cell_type":"markdown","source":["In this tensor : torch.Size([2, 7, 768])\n","The first element 2 represent the batch size , number of inputs/sentences\n","The second element 7 represent the sequence length i.e size of the token including the padding token since it was true (this size [ 101, 2023, 2003,2025,  102,    0,    0] \n","or this one [ 101, 2023, 2003, 1037, 7953, 1012,  102])\n","The third element 768 is the hidden size of the model used distilbert-base-uncased-finetuned-sst-2-english\""],"metadata":{"id":"06BpudNr4hsI"}},{"cell_type":"code","source":["outputs[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v9tbVoakehfa","executionInfo":{"status":"ok","timestamp":1678904023045,"user_tz":360,"elapsed":151,"user":{"displayName":"Dhananjay Kumar","userId":"03056671537097392811"}},"outputId":"142a16ab-f5c6-4444-bc21-28c66b999df1"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 0.0911, -0.0235,  0.4332,  ...,  0.1353,  0.1840,  0.0500],\n","         [ 0.1566, -0.1957,  0.1535,  ...,  0.0468,  0.1863,  0.2464],\n","         [ 0.1405, -0.1565,  0.4472,  ..., -0.0150,  0.0844,  0.4713],\n","         ...,\n","         [-0.0295,  0.1492,  0.3712,  ...,  0.0202,  0.1438, -0.2039],\n","         [ 0.1072, -0.3295,  0.3198,  ...,  0.3632,  0.2085, -0.2717],\n","         [ 1.0473,  0.1609,  0.6181,  ...,  0.6987, -0.1660, -0.1045]],\n","\n","        [[-0.3731,  0.4434, -0.3577,  ..., -0.1507, -0.0791,  0.5306],\n","         [-0.6799,  0.4216, -0.1543,  ..., -0.7253, -0.5644,  0.3843],\n","         [-0.5186,  0.4889, -0.1868,  ..., -0.4023, -0.4582,  0.3454],\n","         ...,\n","         [ 0.2535,  0.4698,  0.1167,  ..., -0.0409, -0.1703, -0.1317],\n","         [-0.0814,  0.3816, -0.3894,  ..., -0.4100, -0.1549,  0.4925],\n","         [-0.0900,  0.0998, -0.1883,  ..., -0.2751, -0.1801,  0.2704]]],\n","       grad_fn=<NativeLayerNormBackward0>)"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["<b>What’s the difference between two code chunks below : </b>\n","\n","\n","```\n","# 1. \n","from transformers import DistilBertTokenizerFast, DistilBertModel\n","\n","tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n","tokens = tokenizer.encode('This is a input.', return_tensors='pt')\n","print(\"These are tokens!\", tokens)\n","for token in tokens[0]:\n","    print(\"This are decoded tokens!\", tokenizer.decode([token]))\n"," \n","model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n","print(model.embeddings.word_embeddings(tokens))\n","model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n","print(model.embeddings.word_embeddings(tokens))\n","```\n","\n","\n","\n","\n","```\n","# 2\n","from transformers import AutoTokenizer\n","checkpoint  = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","raw_inputs = ['This is a input.']\n","inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n"," \n","from transformers import AutoModel\n","model = AutoModel.from_pretrained(checkpoint)\n","outputs = model(**inputs)\n","print(outputs.last_hidden_state.shape)\n","```\n","\n","Both code chunks are creating embeddings out of tokens.\n","\n","In the first code chunk, the tokenizer.encode() method is used to encode the input text as a sequence of token IDs, which are then passed to the model.embeddings.word_embeddings() method to obtain the corresponding embedding vectors for each token in the sequence. The model in this code chunk is an instance of DistilBertModel, which is a pre-trained transformer model that can produce contextualized embeddings for input sequences.\n","\n","In the second code chunk, the AutoTokenizer class is used to tokenize and encode the input text as a sequence of token IDs, which are then passed to the AutoModel class to obtain the corresponding embeddings. The AutoModel class automatically loads the appropriate pre-trained model specified by the checkpoint string, which in this case is a DistilBertModel fine-tuned for sentiment analysis on the SST-2 dataset.\n","\n","Both code chunks essentially perform the same task of encoding a text sequence as a sequence of token IDs and then obtaining embeddings for each token using a pre-trained transformer model. The main difference between the two is the use of the AutoTokenizer and AutoModel classes in the second code chunk, which allow for more flexibility in selecting and loading pre-trained models.\n","\n","\n"],"metadata":{"id":"mBA3kys192m0"}},{"cell_type":"markdown","source":["Classification"],"metadata":{"id":"Xb6UY-E1_LDL"}},{"cell_type":"code","source":["# Classification\n","from transformers import AutoModelForSequenceClassification\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","outputs =  model(**inputs)\n","print(outputs.logits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"grPicV_w66wN","executionInfo":{"status":"ok","timestamp":1678911550837,"user_tz":360,"elapsed":1946,"user":{"displayName":"Dhananjay Kumar","userId":"03056671537097392811"}},"outputId":"66d56b12-4ca8-45dd-8d28-23fab6d0f8ab"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-2.7134,  2.7267],\n","        [ 2.8436, -2.2796]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"markdown","source":["<b>Q. What are logits wrt NLP</b>\n","\n","In natural language processing (NLP), the term \"logits\" refers to the raw, unnormalized outputs of a classification model before they are converted into probabilities using a softmax function.\n","\n","Logits are real-valued numbers that represent the degree of confidence that the model has in each possible output class. For example, in a binary classification problem where the two classes are \"positive\" and \"negative\", the model might output a single logit value that represents the degree of confidence that the input belongs to the positive class.\n","\n","Logits can be positive or negative, and their magnitude represents the strength of the evidence for or against a particular class. Larger logits indicate higher confidence in a particular class, while smaller logits indicate lower confidence. In multi-class classification problems, the model outputs a vector of logits, where each element of the vector corresponds to a possible output class.\n","\n","After the logits are computed, they are typically passed through a softmax function, which normalizes them into a probability distribution over the output classes. This allows the model to make a probabilistic prediction about the most likely output class for a given input."],"metadata":{"id":"Fks73u-66Y-B"}},{"cell_type":"code","source":["import torch \n","\n","predictions  = torch.nn.functional.softmax(outputs.logits, dim=-1)\n","print(predictions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uKTNiW6A6gXs","executionInfo":{"status":"ok","timestamp":1678913177804,"user_tz":360,"elapsed":144,"user":{"displayName":"Dhananjay Kumar","userId":"03056671537097392811"}},"outputId":"44dc7e1c-c3d2-4f34-90ad-1f2ad235b9e5"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.0043, 0.9957],\n","        [0.9941, 0.0059]], grad_fn=<SoftmaxBackward0>)\n"]}]},{"cell_type":"code","source":["model.config.id2label"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7MV7JXGvBgd8","executionInfo":{"status":"ok","timestamp":1678913236113,"user_tz":360,"elapsed":7,"user":{"displayName":"Dhananjay Kumar","userId":"03056671537097392811"}},"outputId":"aa0ed0eb-5bb1-4ded-e4a8-c4bcabee3c02"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'LABEL_0', 1: 'LABEL_1'}"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["Based on above label:\n","\n"," First input/sentence - 0.0043*100 = .43 % Label_0  and 99.57 % Label_1\n","\n"," Second input/sentence - 99.41 % Label_0  and 0.59% % Label_1"],"metadata":{"id":"q4P4kwqGB7mq"}},{"cell_type":"code","source":["from transformers import pipeline\n","\n","classifier = pipeline(\"sentiment-analysis\")\n","classifier(\n","    [\n","        \"I've been waiting for a HuggingFace course my whole life.\",\n","        \"I hate this so much!\",\n","    ]\n",")"],"metadata":{"id":"W-k4NI6ZDI9a","executionInfo":{"status":"ok","timestamp":1678913619915,"user_tz":360,"elapsed":5982,"user":{"displayName":"Dhananjay Kumar","userId":"03056671537097392811"}},"outputId":"93b2261c-b226-4f92-92c2-cc3826324858","colab":{"base_uri":"https://localhost:8080/","height":154,"referenced_widgets":["ced557bc06754653863e91fa447b55d4","ad9d251a066048cda8637ca55cb9a93f","95979b3dd1774ab18d319790d268da5c","60400048faab49978cc17a83498b91cb","f94922a84a66404e8d2f3781e7702739","fd784ec36985474e8fabf73008779134","2c2b630539544ef88c211b6cf0d71f15","a3ed2fea40274606bfd20ecd9b7a1f29","6705d16b705b427f828a56585d7943d9","4c89811caef04190a6f378703915896c","2802fd919a244c45b708efa2548ac451","32cc2490dd4c4efc9a844951a3f582ef","741a7662fe924e9bb497238fb6dc8504","8a3b73e2cb6b475197335339d7581c1b","24066f5755d54b7da6a0fd356b8a24ad","82a3b79a3baa46ce80831c0a25511917","0ff63995b859497aa0ad07f572cbe095","0abd05f2479c413e8825d393aa7a4eff","7255a9abb8314b7a90bfb59a9f4cb187","5b7cea21398747d0a9f6cfb28a8d194f","71296cd55f88408998fe413858062afc","38c4cdd6a88245a4a169d4a7766b8592"]}},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ced557bc06754653863e91fa447b55d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32cc2490dd4c4efc9a844951a3f582ef"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n"," {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":[],"metadata":{"id":"Ttug8D3KDK-3"},"execution_count":null,"outputs":[]}]}