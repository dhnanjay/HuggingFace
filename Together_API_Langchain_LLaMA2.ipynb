{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhnanjay/HuggingFace/blob/main/Together_API_Langchain_LLaMA2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRYSu48huSUW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbc7c8fa-41d7-459e-b8df-5b6c3f7146e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain huggingface_hub tiktoken\n",
        "!pip -q install --upgrade together"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLaMA 2 on Together API"
      ],
      "metadata": {
        "id": "gvIjaK53dP5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"TOGETHER_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "dNA4TsHpu6OM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain"
      ],
      "metadata": {
        "id": "J-KFB7J_u_3L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5f377dc-60d1-4fe0-834d-d826fd0df94f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.0.263\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://www.github.com/hwchase17/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, async-timeout, dataclasses-json, langsmith, numexpr, numpy, openapi-schema-pydantic, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up Together API\n"
      ],
      "metadata": {
        "id": "HqwsGJDhvAQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import together\n",
        "\n",
        "# set your API key\n",
        "together.api_key = os.environ[\"TOGETHER_API_KEY\"]\n",
        "\n",
        "# list available models and descriptons\n",
        "models = together.Models.list()"
      ],
      "metadata": {
        "id": "B3pqftc7nacA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the first model's name\n",
        "print(models[3]['name']), print(models[52]['name'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yu4U9gZPn-Vg",
        "outputId": "6b79cdae-8a18-4903-e2f3-84e9c9b8b610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EleutherAI/pythia-1b-v0\n",
            "togethercomputer/llama-2-13b\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, model in enumerate(models):\n",
        "    print(idx, model['name'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t2q-9TG5ixV",
        "outputId": "7e8f8a23-e92e-4711-fdfb-ce69979896f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 EleutherAI/gpt-j-6b\n",
            "1 EleutherAI/gpt-neox-20b\n",
            "2 EleutherAI/pythia-12b-v0\n",
            "3 EleutherAI/pythia-1b-v0\n",
            "4 EleutherAI/pythia-2.8b-v0\n",
            "5 EleutherAI/pythia-6.9b\n",
            "6 HuggingFaceH4/starchat-alpha\n",
            "7 NousResearch/Nous-Hermes-13b\n",
            "8 NousResearch/Nous-Hermes-Llama2-13b\n",
            "9 NumbersStation/nsql-6B\n",
            "10 OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\n",
            "11 OpenAssistant/stablelm-7b-sft-v7-epoch-3\n",
            "12 bigcode/starcoder\n",
            "13 databricks/dolly-v2-12b\n",
            "14 databricks/dolly-v2-3b\n",
            "15 databricks/dolly-v2-7b\n",
            "16 google/flan-t5-xl\n",
            "17 huggyllama/llama-13b\n",
            "18 huggyllama/llama-30b\n",
            "19 huggyllama/llama-65b\n",
            "20 huggyllama/llama-7b\n",
            "21 lmsys/fastchat-t5-3b-v1.0\n",
            "22 lmsys/vicuna-13b-v1.3\n",
            "23 lmsys/vicuna-7b-v1.3\n",
            "24 prompthero/openjourney\n",
            "25 runwayml/stable-diffusion-v1-5\n",
            "26 stabilityai/stable-diffusion-2-1\n",
            "27 stabilityai/stable-diffusion-xl-base-1.0\n",
            "28 stabilityai/stablelm-base-alpha-3b\n",
            "29 stabilityai/stablelm-base-alpha-7b\n",
            "30 tatsu-lab/alpaca-7b-wdiff\n",
            "31 timdettmers/guanaco-7b\n",
            "32 togethercomputer/GPT-JT-6B-v1\n",
            "33 togethercomputer/GPT-JT-Moderation-6B\n",
            "34 togethercomputer/GPT-NeoXT-Chat-Base-20B\n",
            "35 togethercomputer/Koala-13B\n",
            "36 togethercomputer/Koala-7B\n",
            "37 togethercomputer/LLaMA-2-7B-32K\n",
            "38 togethercomputer/Pythia-Chat-Base-7B-v0.16\n",
            "39 togethercomputer/RedPajama-INCITE-7B-Base\n",
            "40 togethercomputer/RedPajama-INCITE-7B-Chat\n",
            "41 togethercomputer/RedPajama-INCITE-7B-Instruct\n",
            "42 togethercomputer/RedPajama-INCITE-Base-3B-v1\n",
            "43 togethercomputer/RedPajama-INCITE-Chat-3B-v1\n",
            "44 togethercomputer/RedPajama-INCITE-Instruct-3B-v1\n",
            "45 togethercomputer/codegen2-16B\n",
            "46 togethercomputer/codegen2-7B\n",
            "47 togethercomputer/falcon-40b-instruct\n",
            "48 togethercomputer/falcon-40b\n",
            "49 togethercomputer/falcon-7b-instruct\n",
            "50 togethercomputer/falcon-7b\n",
            "51 togethercomputer/llama-2-13b-chat\n",
            "52 togethercomputer/llama-2-13b\n",
            "53 togethercomputer/llama-2-70b-chat\n",
            "54 togethercomputer/llama-2-70b\n",
            "55 togethercomputer/llama-2-7b-chat\n",
            "56 togethercomputer/llama-2-7b\n",
            "57 togethercomputer/mpt-30b-chat\n",
            "58 togethercomputer/mpt-30b\n",
            "59 togethercomputer/mpt-7b-chat\n",
            "60 togethercomputer/mpt-7b-instruct\n",
            "61 togethercomputer/mpt-7b\n",
            "62 togethercomputer/replit-code-v1-3b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(models[55]['name'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhJYDzrb56_2",
        "outputId": "128bdd6b-6d81-4c0d-cee5-af5f57e45969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "togethercomputer/llama-2-7b-chat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "together.Models.start(\"togethercomputer/llama-2-70b-chat\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdFedq669R1D",
        "outputId": "dfe6cba7-b61c-4c42-e9cc-bd9c9080501f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'success': True,\n",
              " 'value': '026632c004680aa0d545c2f310def6925da42cbe696a564a70b28bd9e5918ab9'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import together\n",
        "\n",
        "import logging\n",
        "from typing import Any, Dict, List, Mapping, Optional\n",
        "\n",
        "from pydantic import Extra, Field, root_validator\n",
        "\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.llms.utils import enforce_stop_tokens\n",
        "from langchain.utils import get_from_dict_or_env\n",
        "\n",
        "class TogetherLLM(LLM):\n",
        "    \"\"\"Together large language models.\"\"\"\n",
        "\n",
        "    model: str = \"togethercomputer/llama-2-70b-chat\"\n",
        "    \"\"\"model endpoint to use\"\"\"\n",
        "\n",
        "    together_api_key: str = os.environ[\"TOGETHER_API_KEY\"]\n",
        "    \"\"\"Together API key\"\"\"\n",
        "\n",
        "    temperature: float = 0.7\n",
        "    \"\"\"What sampling temperature to use.\"\"\"\n",
        "\n",
        "    max_tokens: int = 512\n",
        "    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n",
        "\n",
        "    class Config:\n",
        "        extra = Extra.forbid\n",
        "\n",
        "    @root_validator()\n",
        "    def validate_environment(cls, values: Dict) -> Dict:\n",
        "        \"\"\"Validate that the API key is set.\"\"\"\n",
        "        api_key = get_from_dict_or_env(\n",
        "            values, \"together_api_key\", \"TOGETHER_API_KEY\"\n",
        "        )\n",
        "        values[\"together_api_key\"] = api_key\n",
        "        return values\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Return type of LLM.\"\"\"\n",
        "        return \"together\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        \"\"\"Call to Together endpoint.\"\"\"\n",
        "        together.api_key = self.together_api_key\n",
        "        output = together.Complete.create(prompt,\n",
        "                                          model=self.model,\n",
        "                                          max_tokens=self.max_tokens,\n",
        "                                          temperature=self.temperature,\n",
        "                                          )\n",
        "        text = output['output']['choices'][0]['text']\n",
        "        return text\n"
      ],
      "metadata": {
        "id": "RgbLVmf-o4j7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_llm = TogetherLLM(\n",
        "    model= \"togethercomputer/llama-2-70b-chat\",\n",
        "    temperature=0.1,\n",
        "    max_tokens=512\n",
        ")"
      ],
      "metadata": {
        "id": "gnz69LrUpZ5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(test_llm), test_llm.model, test_llm.temperature"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bua4p_K0qEYR",
        "outputId": "fe79275d-e612-4166-b00d-fd7f23966ffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(__main__.TogetherLLM, 'togethercomputer/llama-2-70b-chat', 0.1)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_llm(\"What are the olympics? \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "8CbBzoBgqLCi",
        "outputId": "ba3c4932-201d-467c-db55-48efbe911abf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The Olympics are a major international sporting event that takes place every four years. The Olympic Games are considered the world's foremost sports competition. More than 200 countries and 10,000 athletes participate in the Games, which are held over a period of several weeks. The Olympic Games are a symbol of the world's diverse cultures coming together in a celebration of athletic achievement.\\n\\nThe Olympic Games have a rich history. The first recorded Olympic Games took place in 776 B.C. in ancient Greece. The Games were held every four years for over 1,000 years until they were abolished in 393 A.D. by the Roman Emperor Theodosius. In 1894, the French educator Pierre de Coubertin founded the International Olympic Committee (IOC), which led to the revival of the Olympic Games. The first modern Olympic Games were held in 1896 in Athens, Greece.\\n\\nThe Olympic Games are divided into two main categories: summer and winter games. The Summer Olympics feature sports such as track and field, gymnastics, swimming, and basketball. The Winter Olympics feature sports such as alpine skiing, figure skating, ice hockey, and snowboarding.\\n\\nThe Olympic Games have evolved over the years to include new sports and events. For example, the 2020 Summer Olympics in Tokyo, Japan, featured sports such as surfing, skateboarding, and karate, which were not included in previous Games. The Olympics have also become more inclusive, with the addition of events such as para-sports and women's boxing.\\n\\nThe Olympic Games are a symbol of international unity and athletic achievement. They provide a platform for athletes from around the world to showcase their skills and compete against the best in their sport. The Games also promote the values of friendship, respect, and fair play, and they inspire people of all ages and backgrounds to become involved in sports and lead healthy, active lifestyles.\\n\\n\\n\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain"
      ],
      "metadata": {
        "id": "EDBSj2Bv03mQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import textwrap\n",
        "\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
        "\n",
        "\n",
        "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
        "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
        "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "    return prompt_template\n",
        "\n",
        "def cut_off_text(text, prompt):\n",
        "    cutoff_phrase = prompt\n",
        "    index = text.find(cutoff_phrase)\n",
        "    if index != -1:\n",
        "        return text[:index]\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "def remove_substring(string, substring):\n",
        "    return string.replace(substring, \"\")\n",
        "\n",
        "\n",
        "def parse_text(text):\n",
        "        wrapped_text = textwrap.fill(text, width=100)\n",
        "        print(wrapped_text +'\\n\\n')\n",
        "        # return assistant_text\n"
      ],
      "metadata": {
        "id": "3H7ZINSIqSyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate,  LLMChain"
      ],
      "metadata": {
        "id": "SP4Bk5YBf1mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
        "\n",
        "llm = TogetherLLM(\n",
        "    model= \"togethercomputer/llama-2-70b-chat\",\n",
        "    temperature=0.1,\n",
        "    max_tokens=512\n",
        ")"
      ],
      "metadata": {
        "id": "LL7JGQ5iCzIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are an advanced assistant that excels at translation. \"\n",
        "instruction = \"Convert the following text from English to French:\\n\\n {text}\"\n",
        "template = get_prompt(instruction, system_prompt)\n",
        "print(template)\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTBrt6XmuY3Y",
        "outputId": "5365db26-279b-41ad-95a1-63ee1195fdb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]<<SYS>>\n",
            "You are an advanced assistant that excels at translation. \n",
            "<</SYS>>\n",
            "\n",
            "Convert the following text from English to French:\n",
            "\n",
            " {text}[/INST]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "id": "0SWQ8DT8AyXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"how are you today?\"\n",
        "output = llm_chain.run(text)\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0fUGp-muY6f",
        "outputId": "35b3b86a-bc36-4eff-d5f2-92bdff885d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Sure, here's the translation:\n",
            "\n",
            "Comment allez-vous aujourd'hui?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarization"
      ],
      "metadata": {
        "id": "e38eSqFb3hET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Summarize the following article for me {text}\"\n",
        "system_prompt = \"You are an expert and summarization and expressing key ideas succintly\"\n",
        "\n",
        "template = get_prompt(instruction, system_prompt)\n",
        "print(template)\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dt4mn5CJuY90",
        "outputId": "1cc9e8d6-9d0a-4be3-a58c-e2bd69a8be3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]<<SYS>>\n",
            "You are an expert and summarization and expressing key ideas succintly\n",
            "<</SYS>>\n",
            "\n",
            "Summarize the following article for me {text}[/INST]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_words(input_string):\n",
        "    words = input_string.split(\" \")\n",
        "    return len(words)\n",
        "\n",
        "text = '''Twitter (now X) CEO Linda Yaccarino claims usage at ‘all time high’ in memo to staff\n",
        "Twitter’s (now X’s) newly established CEO Linda Yaccarino touts the company’s success and X’s future plans in a company-wide memo obtained by CNBC. The exec once again claims, without sharing any specific metrics, that the service’s usage is at an “all time high,” and hints at what’s to come in terms of new product experiences for the newly rebranded platform.\n",
        "\n",
        "The service formerly known as Twitter has been working to become more than just a social network and more of an “everything app,” as owner Elon Musk dubbed it.\n",
        "\n",
        "As the Telsa and Space X exec explained in October 2022, telegraphing Twitter’s eventual rebranding, buying Twitter was meant to be “an accelerant to creating X, the everything app.”\n",
        "\n",
        "\n",
        "His grand plan has been to create an app that allows creators to monetize their content, then later moves into payments services and even banking, Musk remarked during a Twitter Spaces livestream with advertisers in November. At the time, he even mentioned the possibility of establishing money market accounts on Twitter that would pay a high-interest rate to attract consumers to X.\n",
        "\n",
        "Those possible product concepts were again referenced in Yaccarino’s new missive, when she writes, “Our usage is at an all time high and we’ll continue to delight our entire community with new experiences in audio, video, messaging, payments, banking – creating a global marketplace for ideas, goods, services, and opportunities.”\n",
        "\n",
        "Twitter, now X, has already implemented some of Musk’s ideas around videos and creator monetization. In May, the company began allowing subscribers to upload two-hour videos to its service, which advertiser Apple then leveraged when it released the entire first episode of its hit Apple TV+ show “Silo” on the platform. Fired Fox News host Tucker Carlson had been posting lengthy videos to Twitter as well, until ordered to stop by the network.\n",
        "\n",
        "In addition, earlier this month, Twitter began sharing ad revenue with verified creators.\n",
        "\n",
        "However, all is not well at Twitter X, whose traffic — at least by third-party measurements — has been dropping. Data from web analytics firm Similarweb indicated Twitter’s web traffic declined 5% for the first two days its latest rival, Instagram Threads, became generally available, compared with the week prior. Plus, Similarweb said Twitter’s web traffic was down 11% compared with the same days in 2022. Additionally, Cloudflare CEO Matthew Prince earlier this month tweeted a graph of traffic to the Twitter.com domain that showed “Twitter traffic tanking,” he said.\n",
        "\n",
        "\n",
        "Yaccarino subtly pushed back at those reports at the time, claiming that Twitter had its largest usage day since February in early July. She did not share any specific metrics or data. At the same time, however, the company was quietly blocking links to Threads.net in Twitter searches, suggesting it was concerned about the new competition.\n",
        "\n",
        "Today, Yaccarino repeats her vague claims around X’s high usage in her company-wide memo even as analysts at Forrester are predicting X will either shut down or be acquired within the next 12 months and numerous critics concur that the X rebrand is destined to fail.\n",
        "\n",
        "Yaccarino’s memo, otherwise, was mostly a lot of cheerleading, applauding X’s team for their work and touting X’s ability to “impress the world all over again,” as Twitter once did.\n",
        "\n",
        "The full memo, courtesy of CBNC, is below:\n",
        "\n",
        "Hi team,\n",
        "\n",
        "What a momentous weekend. As I said yesterday, it’s extremely rare, whether it’s in life or in business, that you have the opportunity to make another big impression. That’s what we’re experiencing together, in real time. Take a moment to put it all into perspective.\n",
        "\n",
        "17 years ago, Twitter made a lasting imprint on the world. The platform changed the speed at which people accessed information. It created a new dynamic for how people communicated, debated, and responded to things happening in the world. Twitter introduced a new way for people, public figures, and brands to build long lasting relationships. In one way or another, everyone here is a driving force in that change. But equally all our users and partners constantly challenged us to dream bigger, to innovate faster, and to fulfill our great potential.\n",
        "\n",
        "With X we will go even further to transform the global town square — and impress the world all over again.\n",
        "\n",
        "Our company uniquely has the drive to make this possible. Many companies say they want to move fast — but we enjoy moving at the speed of light, and when we do, that’s X. At our core, we have an inventor mindset — constantly learning, testing out new approaches, changing to get it right and ultimately succeeding.\n",
        "\n",
        "With X, we serve our entire community of users and customers by working tirelessly to preserve free expression and choice, create limitless interactivity, and create a marketplace that enables the economic success of all its participants.\n",
        "\n",
        "The best news is we’re well underway. Everyone should be proud of the pace of innovation over the last nine months — from long form content, to creator monetization, and tremendous advancements in brand safety protections. Our usage is at an all time high and we’ll continue to delight our entire community with new experiences in audio, video, messaging, payments, banking – creating a global marketplace for ideas, goods, services, and opportunities.\n",
        "\n",
        "Please don’t take this moment for granted. You’re writing history, and there’s no limit to our transformation. And everyone, is invited to build X with us.\n",
        "\n",
        "Elon and I will be working across every team and partner to bring X to the world. That includes keeping our entire community up to date, ensuring that we all have the information we need to move forward.\n",
        "\n",
        "Now, let’s go make that next big impression on the world, together.\n",
        "\n",
        "Linda'''\n",
        "\n",
        "count_words(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gykQOf4OuZBK",
        "outputId": "ab5d066e-9d0e-46f7-82e6-cada14234918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "940"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm_chain.run(text)\n",
        "print(count_words(output))\n",
        "parse_text(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4VDYh3VuZD_",
        "outputId": "9da5d7a3-2b34-4323-d437-62fbf29b0e9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "98\n",
            " Linda Yaccarino, CEO of Twitter (now rebranded as X), claims that the platform's usage is at an\n",
            "\"all-time high\" in a company-wide memo, despite reports of declining traffic. She touts the\n",
            "company's success and future plans, including new product experiences in audio, video, messaging,\n",
            "payments, and banking. The memo follows Elon Musk's vision for X as an \"everything app\" and his\n",
            "plans to create a global marketplace for ideas, goods, and services. However, some analysts predict\n",
            "that X will shut down or be acquired within the next 12 months, and critics question the success of\n",
            "the rebrand.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Chatbot"
      ],
      "metadata": {
        "id": "MsPFAOBK74Yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain import LLMChain, PromptTemplate"
      ],
      "metadata": {
        "id": "Inghmejz_Tkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Chat History:\\n\\n{chat_history} \\n\\nHuman: {user_input}\\n\\n Assistant:\"\n",
        "system_prompt = \"You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\"\n",
        "\n",
        "template = get_prompt(instruction, system_prompt)\n",
        "print(template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7DAJoLEAOU7",
        "outputId": "54c65ddf-0539-48ed-8c64-e375ccb42806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]<<SYS>>\n",
            "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
            "<</SYS>>\n",
            "\n",
            "Chat History:\n",
            "\n",
            "{chat_history} \n",
            "\n",
            "Human: {user_input}\n",
            "\n",
            " Assistant:[/INST]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"user_input\"], template=template\n",
        ")\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
      ],
      "metadata": {
        "id": "Qv7iPmbI_TnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True,\n",
        "    memory=memory,\n",
        ")"
      ],
      "metadata": {
        "id": "CGjjiqxz_TqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(user_input=\"Hi, my name is Sam\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "X3VadjbM_Tsx",
        "outputId": "bb80656f-3d62-4a9f-a70c-89c99a1cbcd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
            "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
            "<</SYS>>\n",
            "\n",
            "Chat History:\n",
            "\n",
            " \n",
            "\n",
            "Human: Hi, my name is Sam\n",
            "\n",
            " Assistant:[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hello Sam! It's nice to meet you. Is there something I can help you with or would you like to chat for a bit?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(user_input=\"Can you tell me about yourself.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "ENqnCnun_TvC",
        "outputId": "f7b3dfe3-2f2e-4aa0-e1f6-1f798e10f56c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
            "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
            "<</SYS>>\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: Hi, my name is Sam\n",
            "AI:  Hello Sam! It's nice to meet you. Is there something I can help you with or would you like to chat for a bit? \n",
            "\n",
            "Human: Can you tell me about yourself.\n",
            "\n",
            " Assistant:[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Sure! I'm just an AI, I don't have a personal life or experiences like humans do. However, I can tell you that I'm here to help answer any questions you might have, provide information, or assist with tasks. I'm designed to be a helpful tool for people, and I'm always happy to help in any way I can. Is there something specific you'd like to know or discuss?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(user_input=\"Today is Friday. What number day of the week is that?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "_HIqTiLc_Tx2",
        "outputId": "b0dc3c28-b923-418b-cf4a-05e8b81d9ac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
            "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
            "<</SYS>>\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: Hi, my name is Sam\n",
            "AI:  Hello Sam! It's nice to meet you. Is there something I can help you with or would you like to chat for a bit?\n",
            "Human: Can you tell me about yourself.\n",
            "AI:  Sure! I'm just an AI, I don't have a personal life or experiences like humans do. However, I can tell you that I'm here to help answer any questions you might have, provide information, or assist with tasks. I'm designed to be a helpful tool for people, and I'm always happy to help in any way I can. Is there something specific you'd like to know or discuss? \n",
            "\n",
            "Human: Today is Friday. What number day of the week is that?\n",
            "\n",
            " Assistant:[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Sure, I can help you with that! Today is indeed Friday, which is the fifth day of the week. The number of the day is 5.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(user_input=\"what is the day today?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "r5Kn6p15Ky2F",
        "outputId": "910f02fc-da84-4e99-e7e0-e12699f02284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
            "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
            "<</SYS>>\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: Hi, my name is Sam\n",
            "AI:  Hello Sam! It's nice to meet you. Is there something I can help you with or would you like to chat for a bit?\n",
            "Human: Can you tell me about yourself.\n",
            "AI:  Sure! I'm just an AI, I don't have a personal life or experiences like humans do. However, I can tell you that I'm here to help answer any questions you might have, provide information, or assist with tasks. I'm designed to be a helpful tool for people, and I'm always happy to help in any way I can. Is there something specific you'd like to know or discuss?\n",
            "Human: Today is Friday. What number day of the week is that?\n",
            "AI:  Sure, I can help you with that! Today is indeed Friday, which is the fifth day of the week. The number of the day is 5. \n",
            "\n",
            "Human: what is the day today?\n",
            "\n",
            " Assistant:[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Sure, I can help you with that! Today is Friday.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(user_input=\"What is my name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "CGXX3RoT74qz",
        "outputId": "8a5a85b0-bc53-4813-9f8a-3874afc16a5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
            "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
            "<</SYS>>\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: Hi, my name is Sam\n",
            "AI:  Hello Sam! It's nice to meet you. Is there something I can help you with or would you like to chat for a bit?\n",
            "Human: Can you tell me about yourself.\n",
            "AI:  Sure! I'm just an AI, I don't have a personal life or experiences like humans do. However, I can tell you that I'm here to help answer any questions you might have, provide information, or assist with tasks. I'm designed to be a helpful tool for people, and I'm always happy to help in any way I can. Is there something specific you'd like to know or discuss?\n",
            "Human: Today is Friday. What number day of the week is that?\n",
            "AI:  Sure, I can help you with that! Today is indeed Friday, which is the fifth day of the week. The number of the day is 5.\n",
            "Human: what is the day today?\n",
            "AI:  Sure, I can help you with that! Today is Friday. \n",
            "\n",
            "Human: What is my name?\n",
            "\n",
            " Assistant:[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Sure, I can help you with that! Your name is Sam.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(user_input=\"Can you tell me about the olympics\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "YCBnZHm9BOQU",
        "outputId": "c82bcfc3-46d3-43e3-f842-e9405fefdf7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
            "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
            "<</SYS>>\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: Hi, my name is Sam\n",
            "AI:  Hello Sam! It's nice to meet you. Is there something I can help you with or would you like to chat for a bit?\n",
            "Human: Can you tell me about yourself.\n",
            "AI:  Sure! I'm just an AI, I don't have a personal life or experiences like humans do. However, I can tell you that I'm here to help answer any questions you might have, provide information, or assist with tasks. I'm designed to be a helpful tool for people, and I'm always happy to help in any way I can. Is there something specific you'd like to know or discuss?\n",
            "Human: Today is Friday. What number day of the week is that?\n",
            "AI:  Sure, I can help you with that! Today is indeed Friday, which is the fifth day of the week. The number of the day is 5.\n",
            "Human: what is the day today?\n",
            "AI:  Sure, I can help you with that! Today is Friday.\n",
            "Human: What is my name?\n",
            "AI:  Sure, I can help you with that! Your name is Sam. \n",
            "\n",
            "Human: Can you tell me about the olympics\n",
            "\n",
            " Assistant:[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Sure, I'd be happy to help! The Olympics are a major international sporting event that takes place every four years, featuring a wide variety of sports and athletes from around the world. The Olympic Games have a rich history, dating back to ancient Greece, and have evolved over time to include new sports and events. The Summer Olympics and Winter Olympics are held alternately, with the Summer Olympics featuring sports such as track and field, swimming, and gymnastics, while the Winter Olympics feature sports such as skiing, skating, and ice hockey.\\n\\nThe Olympics are organized by the International Olympic Committee (IOC), which is responsible for selecting the host city, determining the sports and events to be included, and enforcing the rules and regulations of the Games. The Olympics are a symbol of international unity and athletic achievement, and are watched by millions of people around the world.\\n\\nIs there anything specific you'd like to know about the Olympics?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(user_input=\"What have we talked about in this Chat?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "Nf2QRotKBOTq",
        "outputId": "158f62c6-2ed5-427f-9cf1-ac216c6c186b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
            "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
            "<</SYS>>\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: Hi, my name is Sam\n",
            "AI:  Hello Sam! It's nice to meet you. Is there something I can help you with or would you like to chat for a bit?\n",
            "Human: Can you tell me about yourself.\n",
            "AI:  Sure! I'm just an AI, I don't have a personal life or experiences like humans do. However, I can tell you that I'm here to help answer any questions you might have, provide information, or assist with tasks. I'm designed to be a helpful tool for people, and I'm always happy to help in any way I can. Is there something specific you'd like to know or discuss?\n",
            "Human: Today is Friday. What number day of the week is that?\n",
            "AI:  Sure, I can help you with that! Today is indeed Friday, which is the fifth day of the week. The number of the day is 5.\n",
            "Human: what is the day today?\n",
            "AI:  Sure, I can help you with that! Today is Friday.\n",
            "Human: What is my name?\n",
            "AI:  Sure, I can help you with that! Your name is Sam.\n",
            "Human: Can you tell me about the olympics\n",
            "AI:  Sure, I'd be happy to help! The Olympics are a major international sporting event that takes place every four years, featuring a wide variety of sports and athletes from around the world. The Olympic Games have a rich history, dating back to ancient Greece, and have evolved over time to include new sports and events. The Summer Olympics and Winter Olympics are held alternately, with the Summer Olympics featuring sports such as track and field, swimming, and gymnastics, while the Winter Olympics feature sports such as skiing, skating, and ice hockey.\n",
            "\n",
            "The Olympics are organized by the International Olympic Committee (IOC), which is responsible for selecting the host city, determining the sports and events to be included, and enforcing the rules and regulations of the Games. The Olympics are a symbol of international unity and athletic achievement, and are watched by millions of people around the world.\n",
            "\n",
            "Is there anything specific you'd like to know about the Olympics? \n",
            "\n",
            "Human: What have we talked about in this Chat?\n",
            "\n",
            " Assistant:[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Sure, I can help you with that! In this chat, you and I have talked about:\\n\\n1. Your name, which is Sam.\\n2. The day of the week, which is Friday.\\n3. The number of the day, which is 5.\\n4. The Olympics, including a brief overview of the history and organization of the Games, as well as the sports and events that are included.\\n\\nIs there anything else you'd like to know or discuss?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "together.Models.stop(\"togethercomputer/llama-2-70b-chat\")"
      ],
      "metadata": {
        "id": "W4aLRBjEBOW9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c801e319-37de-4a39-8008-d6d3de58d12c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'success': True}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "65Bmvfjk9MOf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}