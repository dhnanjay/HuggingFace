{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhnanjay/HuggingFace/blob/main/Fine_tuned_Llama_PEFT_QLora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Install necessary libraries\n",
        "\n",
        "Transformers - Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models\n",
        "\n",
        "Datasets - Datasets is a library for easily accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks\n",
        "\n",
        "PEFT - Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters\n",
        "\n",
        "trl - a set of tools to train transformer language models. In this case the Supervised Fine-tuning step (SFT)\n",
        "\n",
        "accelerate - Accelerate is a library that enables the same PyTorch code to be run across any distributed configuration by adding just four lines of code\n",
        "\n",
        "bitsandbytes - Library you need to use in order to quantize the LLM"
      ],
      "metadata": {
        "id": "EhG0t101BPda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers\n",
        "!pip install xformers\n",
        "!pip install -q datasets\n",
        "!pip install -q trl\n",
        "!pip install git+https://github.com/huggingface/peft.git\n",
        "!pip install -q bitsandbytes==0.37.2\n",
        "!pip install -q -U accelerate"
      ],
      "metadata": {
        "id": "xOrwlJEGBUSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import following libraries"
      ],
      "metadata": {
        "id": "XpNAMu9vBXjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import bitsandbytes as bnb\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, PeftConfig\n",
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments, pipeline\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "65phwrUEBZMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load a model and tokenizer"
      ],
      "metadata": {
        "id": "b4GpeG7nBbY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to use LLama 2 7B model"
      ],
      "metadata": {
        "id": "MJ5R2uNWBeFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "ZrGKrmhNBfjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"meta-llama/Llama-2-7b-chat-hf\" # Modify to whatever model you want to use\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    repo_id,\n",
        "    device_map='auto',\n",
        "    load_in_8bit=True,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "base_model.config.use_cache = False"
      ],
      "metadata": {
        "id": "leTZ3ihyBiVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(base_model) # use it to check what target module should be"
      ],
      "metadata": {
        "id": "-gt9a_23BpD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.get_memory_footprint() # Check the memory"
      ],
      "metadata": {
        "id": "Ev7o9wSFBwjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper function to see how many parameters the model has:"
      ],
      "metadata": {
        "id": "VyYHkYn0CMuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "HIQhEglTCNpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the base model"
      ],
      "metadata": {
        "id": "wfnsMAtTCRFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\"\n",
        "\n",
        "def user_prompt(human_prompt):\n",
        "    prompt_template=f\"### HUMAN:\\n{human_prompt}\\n\\n### RESPONSE:\\n\" # This has to change if your dataset isn't formatted as Alpaca\n",
        "    return prompt_template\n",
        "\n",
        "pipe = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=base_model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=150,\n",
        "    repetition_penalty=1.15,\n",
        "    top_p=0.95\n",
        "    )\n",
        "result = pipe(user_prompt(\"You are an expert youtuber. Give me some ideas for a youtube title for a video about fine tuning LLM\"))\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "TPi_C-WXCSKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare and preprocess the model for training"
      ],
      "metadata": {
        "id": "85HJ5gQUCWC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"], # you have to know the target modules, it varies from model to model\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "\n",
        "model = get_peft_model(base_model, config) # Wrap the base model with get_peft_model() to get a trainable PeftModel\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "xkz_1eHYCW7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load a dataset from datasets library"
      ],
      "metadata": {
        "id": "9QFktYvAChQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"csv\", data_files = \"you_data_here.csv\") # substitute with whatever file name you have\n",
        "print(\"Dataset loaded\")"
      ],
      "metadata": {
        "id": "iN4kcQ-ICiKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training step"
      ],
      "metadata": {
        "id": "OUQ_cG5oCsmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adam_bits = 8\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir = \"Trainer_output\",\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    run_name=f\"deb-v2-xl-{adam_bits}bitAdam\",\n",
        "    logging_steps = 20,\n",
        "    learning_rate = 2e-4,\n",
        "    fp16=True,\n",
        "    max_grad_norm = 0.3,\n",
        "    max_steps = 300,\n",
        "    warmup_ratio = 0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type = \"constant\",\n",
        ")"
      ],
      "metadata": {
        "id": "ii3FjHhfCtoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = dataset[\"train\"],\n",
        "    dataset_text_field=\"text\",\n",
        "    args = training_arguments,\n",
        "    max_seq_length = 512,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "wyOKARWLCvr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the adapter"
      ],
      "metadata": {
        "id": "oZHxNbOzCyeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"Finetuned_adapter\")\n",
        "adapter_model = model\n",
        "\n",
        "print(\"Lora Adapter saved\")"
      ],
      "metadata": {
        "id": "jQ0xHD1tCzZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge the base model and the adapter"
      ],
      "metadata": {
        "id": "YgRpbB5uC34-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Can't merge the 8 bit/4 bit model with Lora so reload it\n",
        "\n",
        "repo_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "use_ram_optimized_load=False\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    repo_id,\n",
        "    device_map='auto',\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "base_model.config.use_cache = False"
      ],
      "metadata": {
        "id": "JD3vQTcaC6Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.get_memory_footprint()"
      ],
      "metadata": {
        "id": "Qsxf16axC9Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Lora adapter\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    \"/content/Finetuned_adapter\",\n",
        "    )\n",
        "merged_model = model.merge_and_unload()\n",
        "\n",
        "merged_model.save_pretrained(\"/content/Merged_model\")\n",
        "tokenizer.save_pretrained(\"/content/Merged_model\")"
      ],
      "metadata": {
        "id": "CGEHrdc-DAzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing out Fine Tuned model"
      ],
      "metadata": {
        "id": "1bfiNRSADDCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\"\n",
        "\n",
        "def user_prompt(human_prompt):\n",
        "    prompt_template=f\"### HUMAN:\\n{human_prompt}\\n\\n### RESPONSE:\\n\"\n",
        "    return prompt_template\n",
        "\n",
        "pipe = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=merged_model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=150,\n",
        "    repetition_penalty=1.15,\n",
        "    top_p=0.95\n",
        "    )\n",
        "result = pipe(user_prompt(\"You are an expert youtuber. Give me some ideas for a youtube title for a video about fine tuning LLM\"))\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "KfPuoMSrDD5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model.push_to_hub(\"your_hg_id/name_fine_tuned_model\")"
      ],
      "metadata": {
        "id": "1zMZI3HlDPI0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}