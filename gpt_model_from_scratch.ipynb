{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhnanjay/HuggingFace/blob/main/gpt_model_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is heavily inspired from [Sebastian Raschka](https://x.com/rasbt)'s excellent book, \"[Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)\".\n",
        "\n",
        "I highly recommend buying and reading that book.\n",
        "\n",
        "My code here is taken directly from Sebastian's book, but with some slight variable and styling updates.\n",
        "\n",
        "Questions?  Feel free to DM me on [Twitter](https://x.com/virattt)."
      ],
      "metadata": {
        "id": "M5bRPAw1n_oG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken --quiet"
      ],
      "metadata": {
        "id": "HuZEvoAjxYNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Setup the config"
      ],
      "metadata": {
        "id": "Xg68QogFtspy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvXrPrU4q--F"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 1024,\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": False\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Implement \"dummy\" transformer"
      ],
      "metadata": {
        "id": "2IOBuc1MxTNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DummyGPTModel(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.token_embedding = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])          # 50257 x 768\n",
        "    self.position_embedding = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])   # 1024 x 768\n",
        "    self.drop_embedding = nn.Dropout(config[\"drop_rate\"])\n",
        "    self.transformer_blocks = nn.Sequential(\n",
        "        *[DummyTransformerBlock(config)\n",
        "          for _ in range(config[\"n_layers\"])]\n",
        "    )\n",
        "    self.layer_norm = DummyLayerNorm(config[\"emb_dim\"])\n",
        "    self.out_head = nn.Linear(\n",
        "        config[\"emb_dim\"], config[\"vocab_size\"], bias=False\n",
        "    )\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, sequence_length = in_idx.shape\n",
        "    token_embeddings = self.token_embedding(in_idx)\n",
        "    position_embeddings = self.position_embedding(\n",
        "      torch.arange(sequence_length, device=in_idx.device)\n",
        "    )\n",
        "    x = token_embeddings + position_embeddings\n",
        "    x = self.drop_embedding(x)\n",
        "    x = self.transformer_blocks(x)\n",
        "    x = self.layer_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits\n",
        "\n",
        "\n",
        "class DummyTransformerBlock(nn.Module):\n",
        "  def __init__(self, config):\n",
        "      super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x\n",
        "\n",
        "class DummyLayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x"
      ],
      "metadata": {
        "id": "IM5IYm1st8VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Test out \"dummy\" transformer"
      ],
      "metadata": {
        "id": "laEWjyCbxdjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim=0)\n",
        "print(batch)"
      ],
      "metadata": {
        "id": "3xsW7vbhv7pS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = DummyGPTModel(GPT_CONFIG_124M)\n",
        "logits = model(batch)\n",
        "print(f\"Output shape: {logits.shape}\")\n",
        "print(logits)"
      ],
      "metadata": {
        "id": "naqM1lRAxhBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Normalize the vector activations"
      ],
      "metadata": {
        "id": "Duj4gbn5yzfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "batch_example = torch.randn(2, 5)\n",
        "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
        "out = layer(batch_example)\n",
        "print(out)"
      ],
      "metadata": {
        "id": "rK6qBHZLzVHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine mean and variance\n",
        "mean = out.mean(dim=-1, keepdim=True)\n",
        "var = out.var(dim=-1, keepdim=True)\n",
        "print(f\"Mean: {mean} \\n\")\n",
        "print(f\"Variance: {var} \\n\")"
      ],
      "metadata": {
        "id": "snA2wzk8Ma-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_norm = (out - mean) / torch.sqrt(var) # This is also known as the standard deviation\n",
        "mean = out_norm.mean(dim=-1, keepdim=True)\n",
        "var = out_norm.var(dim=-1, keepdim=True)\n",
        "print(\"Normalized layer outputs:\\n\", out_norm)\n",
        "print(\"Mean:\\n\", mean)\n",
        "print(\"Variance:\\n\", var)"
      ],
      "metadata": {
        "id": "ZdE0tgS2Ms-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "    normalized_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "    return self.scale * normalized_x + self.shift"
      ],
      "metadata": {
        "id": "RxG3Sh9NNVYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try out the LayerNorm module\n",
        "ln = LayerNorm(emb_dim=5)\n",
        "out_ln = ln(batch_example)\n",
        "mean = out_ln.mean(dim=-1, keepdim=True)\n",
        "var = out_ln.var(dim=-1, keepdim=True, unbiased=False)\n",
        "print(f\"Mean: {mean} \\n\")\n",
        "print(f\"Variance: {var} \\n\")"
      ],
      "metadata": {
        "id": "GPM9wZORORXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Implement feed forward network with GELU activations"
      ],
      "metadata": {
        "id": "7UMk2RGQPKeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))"
      ],
      "metadata": {
        "id": "loslDi27PNdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "gelu, relu = GELU(), nn.ReLU()\n",
        "\n",
        "x = torch.linspace(-3, 3, 100)\n",
        "y_gelu, y_relu = gelu(x), relu(x)\n",
        "plt.figure(figsize=(8, 3))\n",
        "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
        "    plt.subplot(1, 2, i)\n",
        "    plt.plot(x, y)\n",
        "    plt.title(f\"{label} activation function\")\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(f\"{label}(x)\")\n",
        "    plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F6fGJIjSSIYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement feed-forward neural network\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(config[\"emb_dim\"], 4 * config[\"emb_dim\"]),\n",
        "        GELU(),\n",
        "        nn.Linear(4 * config[\"emb_dim\"], config[\"emb_dim\"]),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "5ROLTWSJW55K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test it out\n",
        "ffn = FeedForward(GPT_CONFIG_124M)\n",
        "x = torch.rand(2, 3, 768) # 2 batches, 3 input examples, 768 embeddings per example\n",
        "out = ffn(x)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "id": "paZpT47CYM-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Implement multi-head attention"
      ],
      "metadata": {
        "id": "TYxPkbofY0gB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "\n",
        "    assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "\n",
        "    self.d_out = d_out                  # 768\n",
        "    self.num_heads = num_heads          # 12\n",
        "    self.head_dim = d_out // num_heads  # 64\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self._out_proj = nn.Linear(d_out, d_out)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer(\n",
        "        'mask',\n",
        "        torch.triu(torch.ones(\n",
        "            context_length,             # 1024\n",
        "            context_length,             # 1024\n",
        "          ), diagonal=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, num_tokens, embedding_length = x.shape\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    # Add the num_heads and head_dim dimensions\n",
        "    keys = keys.view(batch_size, num_tokens, self.num_heads, self.head_dim)       # Transform to a tensor of dimensions: 2 x 1024 x 12 x 64\n",
        "    queries = queries.view(batch_size, num_tokens, self.num_heads, self.head_dim) # Transform to a tensor of dimensions: 2 x 1024 x 12 x 64\n",
        "    values = values.view(batch_size, num_tokens, self.num_heads, self.head_dim)   # Transform to a tensor of dimensions: 2 x 1024 x 12 x 64\n",
        "\n",
        "    # Transpose from (batch_size, num_tokens, num_heads, head_dim) to (batch_size, num_heads, num_tokens, head_dim)\n",
        "    queries = queries.transpose(1, 2)\n",
        "    keys = keys.transpose(1, 2)\n",
        "    values = values.transpose(1, 2)\n",
        "\n",
        "    # Calculate attention scores\n",
        "    attention_scores = queries @ keys.transpose(2, 3)\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "    # Mask the attention scores\n",
        "    attention_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "    # Calculate attention weights\n",
        "    attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "    # Apply dropout to attention weights\n",
        "    attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "    # Calculate context vectors\n",
        "    context_vectors = (attention_weights @ values).transpose(1, 2)\n",
        "\n",
        "    # Concatenate the context vectors\n",
        "    context_vectors = context_vectors.contiguous().view(batch_size, num_tokens, self.d_out)\n",
        "    return self._out_proj(context_vectors)"
      ],
      "metadata": {
        "id": "N3eU4P13Y2ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Implement the Transformer block"
      ],
      "metadata": {
        "id": "jBhdcB3hYXq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attention = MultiHeadAttention(\n",
        "        d_in=config[\"emb_dim\"],\n",
        "        d_out=config[\"emb_dim\"],\n",
        "        context_length=config[\"context_length\"],\n",
        "        dropout=config[\"drop_rate\"],\n",
        "        num_heads=config[\"n_heads\"],\n",
        "        qkv_bias=config[\"qkv_bias\"]\n",
        "    )\n",
        "\n",
        "    self.ff = FeedForward(config)\n",
        "    self.norm1 = LayerNorm(config[\"emb_dim\"])\n",
        "    self.norm2 = LayerNorm(config[\"emb_dim\"])\n",
        "    self.drop_shortcut = nn.Dropout(config[\"drop_rate\"])\n",
        "\n",
        "  def forward(self, x):\n",
        "    shortcut = x\n",
        "\n",
        "    # Attention layer\n",
        "    x = self.norm1(x)\n",
        "    x = self.attention(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut         # Add the original input back\n",
        "\n",
        "    # Feedforward layer\n",
        "    shortcut = x\n",
        "    x = self.norm2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut         # Add the original input back\n",
        "    return x"
      ],
      "metadata": {
        "id": "JnzH0rp4Yg76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate TransformerBlock and feed it some sample data\n",
        "\n",
        "torch.manual_seed(123)\n",
        "x = torch.rand(2, 3, 768)\n",
        "block = TransformerBlock(GPT_CONFIG_124M)\n",
        "out = block(x)\n",
        "\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {out.shape}\")\n",
        "print(f\"Output: {out}\")"
      ],
      "metadata": {
        "id": "U7UOeMQQcTxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Implement the GPT model"
      ],
      "metadata": {
        "id": "KGmpcEtyFWau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.token_embedding = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
        "    self.positional_embedding = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n",
        "    self.drop_embedding = nn.Dropout(config[\"drop_rate\"])\n",
        "\n",
        "    self.transformer_blocks = nn.Sequential(\n",
        "        *[TransformerBlock(config) for _ in range(config[\"n_layers\"])]\n",
        "    )\n",
        "\n",
        "    self.final_norm = LayerNorm(config[\"emb_dim\"])\n",
        "    self.out_head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, sequence_length = in_idx.shape\n",
        "    token_embeddings = self.token_embedding(in_idx)\n",
        "    positional_embeddings = self.positional_embedding(\n",
        "        torch.arange(sequence_length, device=in_idx.device)\n",
        "    )\n",
        "    x = token_embeddings + positional_embeddings\n",
        "    x = self.drop_embedding(x)\n",
        "\n",
        "    x = self.transformer_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "WVfKzAAsgULA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "out = model(batch)\n",
        "print(\"Input batch:\\n\", batch)\n",
        "print(\"\\nOutput shape:\", out.shape)\n",
        "print(out)"
      ],
      "metadata": {
        "id": "pOPKlmP3hLC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Generate some text"
      ],
      "metadata": {
        "id": "tFd1M8lCh2cQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        logits = logits[:, -1, :]\n",
        "        probas = torch.softmax(logits, dim=-1)\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "    # .unsqueeze(0) adds the batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # Remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n"
      ],
      "metadata": {
        "id": "RcSZ7SbThZY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 256,\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": False\n",
        "}\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "4WsoMh8Whynf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_context = \"Every effort moves you\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "id": "GYPj7rLu1snP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Calculate text generation loss"
      ],
      "metadata": {
        "id": "ZKO8DI63h1OM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some sample inputs\n",
        "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
        "                       [40,    1107, 588]])   #  \"I really like\"]"
      ],
      "metadata": {
        "id": "IjS5in8NRdOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some sample targets for inputs\n",
        "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
        "                        [588,  428,  11311]]) #  \" really like chocolate\"]"
      ],
      "metadata": {
        "id": "59kbEcPWRgrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feed inputs into the model\n",
        "with torch.no_grad():\n",
        "  logits = model(inputs)\n",
        "\n",
        "probas = torch.softmax(logits, dim=-1)\n",
        "print(f\"Probas shape: {probas.shape}\")\n",
        "print(f\"Probas: {probas}\")"
      ],
      "metadata": {
        "id": "dWHeMU9DRj_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probas[0, [0], targets[0][0]]"
      ],
      "metadata": {
        "id": "SXP9LdvEkz8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the corresponding token_ids\n",
        "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
        "print(f\"Token IDs:\\n {token_ids}\")"
      ],
      "metadata": {
        "id": "rS9xdGI5Rywa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
        "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
      ],
      "metadata": {
        "id": "plDBHvpzRzue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the target probabilities\n",
        "# The outputs (target_probas_1 and target_probas_2) show the probabilities that the model assigns to the actual target tokens at each position for the first and second sequences\n",
        "text_idx = 0\n",
        "target_probas_1 = probas[\n",
        "    text_idx,           # index of the batch element (e.g., first sentence)\n",
        "    [0, 1, 2],          # positions in the output sequence (our input sequence only has 3 words, hence 0, 1, 2)\n",
        "    targets[text_idx],  # indices of the actual target tokens at these positions\n",
        "]\n",
        "\n",
        "print(\"Text 1:\", target_probas_1)\n",
        "\n",
        "text_idx = 1\n",
        "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
        "print(\"Text 2:\", target_probas_2)"
      ],
      "metadata": {
        "id": "oSoQxJJkSdc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert target probabilities to logs\n",
        "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
        "print(log_probas)"
      ],
      "metadata": {
        "id": "QP61MnDZWkwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the average log probability\n",
        "avg_log_probas = torch.mean(log_probas)\n",
        "print(avg_log_probas)"
      ],
      "metadata": {
        "id": "bzasKXbgcKWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the negative average log probability (cross entropy loss)\n",
        "neg_avg_log_probas = avg_log_probas * -1\n",
        "print(neg_avg_log_probas)"
      ],
      "metadata": {
        "id": "iKi1hs1EcTb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits_flat = logits.flatten(0, 1)\n",
        "targets_flat = targets.flatten()\n",
        "print(\"Flattened logits:\", logits_flat.shape)\n",
        "print(\"Flattened targets:\", targets_flat.shape)"
      ],
      "metadata": {
        "id": "E0e_6P4scYQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Pytorch's helper function for cross entropy loss instead\n",
        "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "uzsH65QPePq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate perplexity, which signifies the num tokens in the vocab the model is unsure about to generate as the next token\n",
        "perplexity = torch.exp(loss)\n",
        "print(perplexity)"
      ],
      "metadata": {
        "id": "SVwuEV1oedYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. Create data loader"
      ],
      "metadata": {
        "id": "rkTjPOZ8fNH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        " # Create a data loader\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        token_ids = tokenizer.encode(txt)\n",
        "\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            # The input chunk\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            # The target chunk is the input chunk, offset by 1 character\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "\n",
        "            # Append chunk to list of chunks\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "EaTc0S57-lgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "JUqEda4w-fY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12. Download our training text"
      ],
      "metadata": {
        "id": "dlU3lxoB-myU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "text_data = response.text"
      ],
      "metadata": {
        "id": "sMLEhiTTar4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_characters = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "print(f\"Total characters: {total_characters}\")\n",
        "print(f\"Total tokens: {total_tokens}\")"
      ],
      "metadata": {
        "id": "JN9XwyUybPEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. Create training / validation data"
      ],
      "metadata": {
        "id": "3zvFDTkAbY3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.90 # 90% of data will be training, 10% will be validation\n",
        "split_index = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_index]\n",
        "val_data = text_data[split_index:]"
      ],
      "metadata": {
        "id": "mJrhBAZpbrDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        ")"
      ],
      "metadata": {
        "id": "m6vDv3rbb0Uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train loader:\")\n",
        "for x, y in train_loader:\n",
        "  print(x.shape, y.shape)\n",
        "\n",
        "print(\"\\nValidation loader:\")\n",
        "for x, y in val_loader:\n",
        "  print(x.shape, y.shape)"
      ],
      "metadata": {
        "id": "Cu36y64xcZBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14. Implement loss functions\n"
      ],
      "metadata": {
        "id": "CakZ8idGcj9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "  \"\"\"\n",
        "  Calculates the loss for a single batch.\n",
        "  \"\"\"\n",
        "  input_batch = input_batch.to(device)\n",
        "  target_batch = target_batch.to(device)\n",
        "\n",
        "  # Run the model\n",
        "  logits = model(input_batch)\n",
        "\n",
        "  # Calculate the loss\n",
        "  loss = torch.nn.functional.cross_entropy(\n",
        "      logits.flatten(0, 1),\n",
        "      target_batch.flatten(),\n",
        "  )\n",
        "  return loss"
      ],
      "metadata": {
        "id": "NUMWkVvfc0d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "  \"\"\"\n",
        "  Calculates the loss for all batches in a data loader.\n",
        "  \"\"\"\n",
        "  total_loss = 0\n",
        "  if len(data_loader) == 0:\n",
        "    return float(\"nan\")\n",
        "  elif num_batches is None:\n",
        "    num_batches = len(data_loader)\n",
        "  else:\n",
        "    num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "  for index, (input_batch, target_batch) in enumerate(data_loader):\n",
        "    if index < num_batches:\n",
        "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "      total_loss += loss.item()\n",
        "    else:\n",
        "      break\n",
        "  return total_loss / num_batches # Return the average across all batches"
      ],
      "metadata": {
        "id": "VCuWJFD-dJPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out the loss function\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  train_loss = calc_loss_loader(train_loader, model, device)\n",
        "  val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(f\"Train loss: {train_loss}\")\n",
        "print(f\"Val loss: {val_loss}\")"
      ],
      "metadata": {
        "id": "Tqg_VRXOdcXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15. Implement training functions"
      ],
      "metadata": {
        "id": "WrrESn4EhUlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "  \"\"\"\n",
        "  Evaluates the model, giving estimate of model's training progress.\n",
        "  \"\"\"\n",
        "  # Put the model in evaluation mode\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      train_loss = calc_loss_loader(\n",
        "          train_loader, model, device, num_batches=eval_iter\n",
        "      )\n",
        "      val_loss = calc_loss_loader(\n",
        "          val_loader, model, device, num_batches=eval_iter\n",
        "      )\n",
        "  # Put the model back in training mode\n",
        "  model.train()\n",
        "  return train_loss, val_loss"
      ],
      "metadata": {
        "id": "nOiliOoniadv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "  \"\"\"\n",
        "  Generates a sample text using the model.\n",
        "  \"\"\"\n",
        "  # Put the model in evaluation mode\n",
        "  model.eval()\n",
        "  context_size = model.positional_embedding.weight.shape[0]\n",
        "  encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "  with torch.no_grad():\n",
        "      token_ids = generate_text_simple(\n",
        "          model=model, idx=encoded,\n",
        "          max_new_tokens=50, context_size=context_size\n",
        "      )\n",
        "      decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "      print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "  # Put the model back in training mode\n",
        "  model.train()"
      ],
      "metadata": {
        "id": "Z3FGeN2-k8LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_simple(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    device,\n",
        "    num_epochs,\n",
        "    eval_freq,\n",
        "    eval_iter,\n",
        "    start_context,\n",
        "    tokenizer,\n",
        "):\n",
        "  \"\"\"\n",
        "  Trains the model.\n",
        "  \"\"\"\n",
        "  # Initialize variables\n",
        "  train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "  tokens_seen, global_step = 0, -1\n",
        "\n",
        "  # The main training loop\n",
        "  for epoch in range(num_epochs):\n",
        "    # Put model in training mode\n",
        "    model.train()\n",
        "\n",
        "    # Iterate through batches\n",
        "    for input_batch, target_batch in train_loader:\n",
        "      # Reset loss gradients from previous epoch\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Calculate loss for batch\n",
        "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "\n",
        "      # Calculate loss gradients using backprop\n",
        "      loss.backward()\n",
        "\n",
        "      # Update model weights using loss gradients\n",
        "      optimizer.step()\n",
        "\n",
        "      # Update global variables\n",
        "      tokens_seen += input_batch.numel()\n",
        "      global_step += 1\n",
        "\n",
        "      # Evaluation step (optional)\n",
        "      if global_step % eval_freq == 0:\n",
        "        # Evaluate the model\n",
        "        train_loss, val_loss = evaluate_model(\n",
        "            model, train_loader, val_loader, device, eval_iter\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        track_tokens_seen.append(tokens_seen)\n",
        "        # print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "        #       f\"Train loss {train_loss:.3f}, \"\n",
        "        #       f\"Val loss {val_loss:.3f}\"\n",
        "        # )\n",
        "\n",
        "    generate_and_print_sample(\n",
        "        model, tokenizer, device, start_context\n",
        "    )\n",
        "  return train_losses, val_losses, track_tokens_seen\n"
      ],
      "metadata": {
        "id": "SHKm3TzZheOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 16. Train the model"
      ],
      "metadata": {
        "id": "ttNR5YMDl7AL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=0.0004,\n",
        "    weight_decay=0.1,\n",
        ")\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "start_time = time.time()\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    device,\n",
        "    num_epochs=num_epochs,\n",
        "    eval_freq=5,\n",
        "    eval_iter=1,\n",
        "    start_context=\"Oh Juliet, where is\",\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "end_time = time.time()\n",
        "print(f\"Training time: {end_time - start_time}\")"
      ],
      "metadata": {
        "id": "n7ny_Rn-l891"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(\n",
        "        epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
        "    )\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax2 = ax1.twiny()\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "id": "wPYbiSbumU5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\")"
      ],
      "metadata": {
        "id": "Y6SVncgcnoUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params_gpt2 = (\n",
        "    total_params - sum(p.numel()\n",
        "    for p in model.out_head.parameters())\n",
        ")\n",
        "print(f\"Number of trainable parameters \"\n",
        "      f\"considering weight tying: {total_params_gpt2:,}\"\n",
        ")"
      ],
      "metadata": {
        "id": "YswtvXvTnzwz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}