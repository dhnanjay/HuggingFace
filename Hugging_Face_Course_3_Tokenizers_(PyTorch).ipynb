{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhnanjay/HuggingFace/blob/main/Hugging_Face_Course_3_Tokenizers_(PyTorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAvcEB5ocJZJ"
      },
      "source": [
        "# Tokenizers (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Hugging Face Ch 2 Tokenizer](https://huggingface.co/course/chapter2/4?fw=pt)"
      ],
      "metadata": {
        "id": "82DIdgRmg3KN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJwi2SnPcJZL"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kNUZkdMScJZM"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets evaluate transformers[sentencepiece]\n",
        "# !pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizers are one of the core components of the NLP pipeline. **They serve one purpose: to translate text into data that can be processed by the model.** Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we’ll explore exactly what happens in the tokenization pipeline.\n",
        "\n",
        "In NLP tasks, the data that is generally processed is raw text. Here’s an example of such text:\n",
        "\n",
        "\n",
        "```\n",
        "Jim Henson was a puppeteer\n",
        "```\n",
        "\n",
        "However, models can only process numbers, so **we need to find a way to convert the raw text to numbers. That’s what the tokenizers do**, and there are a lot of ways to go about this. The goal is to find the most meaningful representation — that is, the one that makes the most sense to the model — and, if possible, the smallest representation."
      ],
      "metadata": {
        "id": "5VhN8LDWd_r-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PHYwkQ4icJZN",
        "outputId": "d0acb0c3-fa66-48cb-f69e-3373d06cd8ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Jim', 'Henson', 'was', 'a', 'puppeteer']\n"
          ]
        }
      ],
      "source": [
        "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
        "print(tokenized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Loading and saving</h3>\n",
        "\n",
        "Loading and saving tokenizers is as simple as it is with models. Actually, it’s based on the same two methods: <b>from_pretrained() and save_pretrained().</b> These methods will load or save the algorithm used by the tokenizer (a bit like the architecture of the model) as well as its vocabulary (a bit like the weights of the model).\n",
        "\n",
        "Loading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model, except we use the BertTokenizer class:"
      ],
      "metadata": {
        "id": "aKOviUDLb8dw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtviHd6HcJZO"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to AutoModel, the AutoTokenizer class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint:"
      ],
      "metadata": {
        "id": "ThzupzOkcOY2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6wOeoSUcJZP"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBfpcXYdcJZQ",
        "outputId": "98638499-101c-4d13-890d-7436a06d43f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],\n",
              " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer(\"Using a Transformer network is simple\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81ZAfWRfcJZQ"
      },
      "outputs": [],
      "source": [
        "tokenizer.save_pretrained(\"directory_on_my_computer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Encoding</h3>\n",
        "\n",
        "Translating text to numbers is known as encoding. Encoding is done in a two-step process: the tokenization, followed by the conversion to input IDs.\n",
        "\n",
        "As we’ve seen, the first step is to split the text into words (or parts of words, punctuation symbols, etc.), usually called tokens. There are multiple rules that can govern that process, which is why we need to instantiate the tokenizer using the name of the model, to make sure we use the same rules that were used when the model was pretrained.\n",
        "\n",
        "The second step is to convert those tokens into numbers, so we can build a tensor out of them and feed them to the model. To do this, the tokenizer has a vocabulary, which is the part we download when we instantiate it with the from_pretrained() method. Again, we need to use the same vocabulary used when the model was pretrained.\n",
        "\n",
        "To get a better understanding of the two steps, we’ll explore them separately. Note that we will use some methods that perform parts of the tokenization pipeline separately to show you the intermediate results of those steps, but in practice, you should call the tokenizer directly on your inputs (as shown in the section 2).\n",
        "\n",
        "<h3>Tokenization</h3>\n",
        "\n",
        "The tokenization process is done by the tokenize() method of the tokenizer:"
      ],
      "metadata": {
        "id": "zUIA1-Gnf5U_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJmsIKAwcJZR",
        "outputId": "c36a0a90-f307-49c8-ff79-19aa9a3c5494"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence = \"Using a Transformer network is simple\"\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zZu5GNt-cJZS",
        "outputId": "a61d2acb-2574-4663-8aec-c9ad5fbd9917",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ad1cf198a774>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ],
      "source": [
        "# Encoding\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Decoding</h3>\n",
        "\n",
        "Decoding is going the other way around: from vocabulary indices, we want to get a string. This can be done with the decode() method as below:\n",
        "\n",
        "\n",
        "Note that the decode method not only converts the indices back to tokens, but also groups together the tokens that were part of the same words to produce a readable sentence. This behavior will be extremely useful when we use models that predict new text (either text generated from a prompt, or for sequence-to-sequence problems like translation or summarization)."
      ],
      "metadata": {
        "id": "gJCcMoL7hSpJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbRWP2tpcJZT",
        "outputId": "63ef5450-7eca-496b-a806-7d752c209d16"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Using a Transformer network is simple'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
        "print(decoded_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization, encoding, and embedding are all fundamental steps in natural language processing (NLP) that involve converting raw text data into a numerical format that can be used as input for machine learning models.\n",
        "\n",
        "Tokenization involves breaking down a sequence of text into individual tokens or words, usually using a tokenizer library or algorithm.\n",
        "\n",
        "Encoding refers to the process of mapping each token to a unique numerical identifier or index. This is typically done using a lookup table, where each token is associated with a unique integer value.\n",
        "\n",
        "Embedding is the process of representing each token as a dense vector or embedding. These embeddings are learned by a neural network model, and they capture the semantic meaning of the token in a low-dimensional space. Embeddings are typically used as input features for downstream NLP tasks such as sentiment analysis, text classification, or language translation.\n",
        "\n",
        "In summary, tokenization breaks down text into individual tokens, encoding maps these tokens to unique integer values, and embedding represents each token as a dense vector in a lower-dimensional space."
      ],
      "metadata": {
        "id": "5WpdSCEsgmsc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FJN1q-aggpR4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}