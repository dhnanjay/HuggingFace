{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhnanjay/HuggingFace/blob/main/Teach_O_Matic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Teach-O-Matic Channel](https://user-images.githubusercontent.com/14149230/236333210-36ad530d-af79-402c-8297-2e4eaf8b678e.png)](https://www.youtube.com/channel/UCtvf4LKQpFkrUHJ8o3bI8DA)\n"
      ],
      "metadata": {
        "id": "YMz_5TmHbUS5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcRrbtmSkoVR"
      },
      "source": [
        "# üìö Teach-O-Matic\n",
        "\n",
        "In this notebook, we'll create instructional \"how to videos\" on any topic, using LangChain, OpenAI, and Replicate. We created a YouTube channel featuring outputs from this notebook so you get a taste of what you can create:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> üçø Watch [How to Make Spaghetti (By Shakespeare)](https://www.youtube.com/watch?v=mZoGDUckhOE) (1min)\n"
      ],
      "metadata": {
        "id": "iYQCmlLyb4gD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make your own videos, keep running the cells in the notebook!"
      ],
      "metadata": {
        "id": "scNXJ1dhcm4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Table of Contents"
      ],
      "metadata": {
        "id": "-FylA4F5cixH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "toc",
        "id": "kLOpTMwnlCKH"
      },
      "source": [
        ">[üìö Teach-O-Matic](#scrollTo=tcRrbtmSkoVR)\n",
        "\n",
        ">[üåê Links](#scrollTo=2LUvZTDzE-4h)\n",
        "\n",
        ">[üíæ Install Stuff](#scrollTo=OniyJ8XRWQbY)\n",
        "\n",
        ">[‚õìÔ∏è Create our LangChain](#scrollTo=Ou-40KJur3ZI)\n",
        "\n",
        ">[‚ùî What should the video be about?](#scrollTo=ynqnG0SR3nwW)\n",
        "\n",
        ">[üëü Run the Chain!](#scrollTo=LyQwZW-24M10)\n",
        "\n",
        ">[‚è≥ Wait for our async predictions to complete](#scrollTo=Znto050hQCjM)\n",
        "\n",
        ">[ü™° Stitch them all together!](#scrollTo=wkvtnprozZyQ)\n",
        "\n",
        ">[üçø Watch the video](#scrollTo=YWwsGXaVhgwF)\n",
        "\n",
        ">[üß† Things we learned:](#scrollTo=or_M30Ck7Hxb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LUvZTDzE-4h"
      },
      "source": [
        "# üåê Links"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD82tZvOc5hU"
      },
      "source": [
        "- [LangChain](https://python.langchain.com/en/latest/), for chaining together our model requests.\n",
        "- [Replicate](https://replicate.com), for running:\n",
        "  - [suno-ai/bark](https://github.com/suno-ai/bark) for creating the narrator\n",
        "  - [damo-text-to-video](https://replicate.com/cjwbw/damo-text-to-video), to create the videos\n",
        "  - [stable diffusion](https://replicate.com/stability-ai/stable-diffusion), for creating the title/ending screen image\n",
        "  - [riffusion](https://replicate.com/riffusion/riffusion), for creating the background music\n",
        "- [OpenAI Docs](https://beta.openai.com) for creating the script + video descriptions\n",
        "- [MoviePy](https://zulko.github.io/moviepy/) for stitching the video and audio together\n",
        "- [Teach-O-Matic Youtube Channel](https://www.youtube.com/@teach-o-matic). Tweet us [@replicatehq](https://twitter.com/replicatehq) and we'll add your how to video to the channel!\n",
        "- [GitHub Repo](https://github.com/cbh123/teach-o-matic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OniyJ8XRWQbY"
      },
      "source": [
        "# üíæ Install Stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I85vONvgkvEq"
      },
      "outputs": [],
      "source": [
        "!pip install replicate\n",
        "!pip install requests\n",
        "!pip install openai\n",
        "!pip install langchain\n",
        "!pip install moviepy\n",
        "from google.colab import output\n",
        "output.clear()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eEK6sxN5kYE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import time\n",
        "import numpy as np\n",
        "import replicate "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIr-fPFn_FcV"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain, TransformChain\n",
        "from langchain.llms import OpenAI, Replicate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OUg8ixqown0",
        "outputId": "a14b81e3-57f5-4a7e-8b62-6e5fbe0ba27c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "# get your token from https://replicate.com/account\n",
        "from getpass import getpass\n",
        "\n",
        "REPLICATE_API_TOKEN = getpass()\n",
        "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVu7s3DF5mMa",
        "outputId": "7014e1a3-0ba1-4bff-e0b7-de8e92533188"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "# get your key from https://platform.openai.com/account/api-keys\n",
        "OPENAI_API_KEY = getpass()\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou-40KJur3ZI"
      },
      "source": [
        "# ‚õìÔ∏è Create our LangChain\n",
        "First let's generate the script, video descriptions, and title for our how-to video. [LangChain](https://langchain.readthedocs.io/) makes this easy, because we can separate each LLM call into a composable chain.\n",
        "\n",
        "In this step we want to accomplish three things:\n",
        "1. a script that our narrator ([Bark](https://replicate.com/suno-ai/bark)) will read\n",
        "2. a title for our video\n",
        "3. visual descriptions for our script that we run in the [text-to-video](https://replicate.com/cjwbw/damo-text-to-video) model\n",
        "\n",
        "Down the road, we may want to add or change our outputs. Changing our outputs is easy with LangChain ‚Äî‚Äî all it requires is adding or updating an LLMChain.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8DvQ8Ny_mtM"
      },
      "outputs": [],
      "source": [
        "# LLMChain to write a a script for our how-to video.\n",
        "topic_template = \"\"\"\n",
        "Write me the instructions for {topic}. \n",
        "\n",
        "Rules\n",
        "- The output should 6 paragraphs, each no more than 20 words. \n",
        "- Split each output with a \\n\\n\n",
        "- Omit the word 'paragraph'. \n",
        "- The first paragraph should introduce some background on why the problem is worth solving.\n",
        "\n",
        "You are a {narrator_adjectives} narrator.\n",
        "\"\"\"\n",
        "\n",
        "system_message_prompt = SystemMessage(content=\"You are a helpful assistant that enthusiastically teaches people new topics.\")\n",
        "human_message_prompt = HumanMessagePromptTemplate(prompt=PromptTemplate(\n",
        "                                                  template=topic_template,\n",
        "                                                  input_variables=[\"topic\", \"narrator_adjectives\"]))\n",
        "\n",
        "# create the initial script\n",
        "chat = ChatOpenAI(temperature=0.9, model=\"gpt-4\")\n",
        "chat_prompt_template = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "script_chain = LLMChain(llm=chat, prompt=chat_prompt_template, output_key='script')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zUi7iQ7OGfS"
      },
      "outputs": [],
      "source": [
        "# LLMChain to write a title for our video\n",
        "llm = OpenAI(temperature=.9)\n",
        "template = \"\"\"Please come up with a creative and zany title for the below how-to video script. \n",
        "Puns are encouraged. Don't include quotations (\") in the output.\n",
        "\n",
        "Script:\n",
        "{script}\n",
        "Title: \"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"script\"], template=template)\n",
        "title_chain = LLMChain(llm=llm, prompt=prompt_template, output_key='title')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifVZOyErEILX"
      },
      "outputs": [],
      "source": [
        "# LLMChain to write a descriptions of each video that we'll create using a text to video model.\n",
        "video_template = \"\"\"\n",
        "          Create a visual description for each of the 6 following paragraphs. Keep each one as concrete as possible. \n",
        "          \n",
        "          Rules\n",
        "          - Each description should be no more than 20 words. \n",
        "          - The output MUST be very specific and concrete and include a single action.\n",
        "          - Do not include numbers in the output.\n",
        "          - Split each output with a \\n\\n\n",
        "          - Include common household objects\n",
        "\n",
        "          Example input:\n",
        "          Driving a car provides independence and convenience.\\n\\nAdjust seat, mirrors, and steering wheel for comfort.\\n\\nFamiliarize yourself with pedals, gearshift, and dashboard controls.\\n\\nStart the car, press brake, and shift gears.\\n\\nObserve traffic rules, signals, and road signs attentively.\\n\\nPractice regularly to build confidence and improve skills.\n",
        "\n",
        "          Example output:\n",
        "          Happy driver driving a car.\\n\\nDriver adjusting seat and mirrors in car.\\n\\nDriver studying pedals, gearshift, and dashboard.\\n\\nStarting car, pressing brake, and shifting gears.\\n\\nDriver obeying traffic rules and road signs.\\n\\nCar driving on road.\n",
        "          \n",
        "          \"{script}\"\"\n",
        "          \"\"\"\n",
        "\n",
        "system_message_prompt = SystemMessage(content=\"You are a helpful assistant that is helping us create visual depictions of text. Sometimes it's important to see the person doing the action. For example, cooking spaghetti may involve a chef. Fixing a car may involve a mechanic.\")\n",
        "human_message_prompt = HumanMessagePromptTemplate(prompt=PromptTemplate(\n",
        "                                                  template=video_template,\n",
        "                                                  input_variables=[\"script\"]\n",
        "                                                  ))\n",
        "\n",
        "# create the video descriptions\n",
        "chat_prompt_template = ChatPromptTemplate(messages=[system_message_prompt, human_message_prompt], input_variables=[\"script\"], partial_variables={\"format_instructions\": \"Your response should be a list of \\n separated values, eg: `foo\\nbar\\nbaz`\"})\n",
        "video_descriptions_chain = LLMChain(llm=chat, prompt=chat_prompt_template, output_key='video_descriptions')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fl38V4HSISrl"
      },
      "outputs": [],
      "source": [
        "# let's define a new output parser!\n",
        "from langchain.output_parsers.list import ListOutputParser\n",
        "from typing import List\n",
        "\n",
        "class DoubleNewlineOutputParser(ListOutputParser):\n",
        "  \"\"\"Parse out \\n\\n separated lists.\"\"\"\n",
        "  def get_format_instructions(self) -> str:\n",
        "      return (\n",
        "          \"Your response should be a list of \\n\\n separated values, \"\n",
        "          \"eg: `foo\\n\\nbar\\n\\nbaz`\"\n",
        "      )\n",
        "\n",
        "  def parse(self, text: str) -> List[str]:\n",
        "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
        "        return text.strip().split(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GH99FV1x-agB"
      },
      "outputs": [],
      "source": [
        "# LLMChain to create the replicate predictions for our text-to-video model\n",
        "def transform_func(inputs: dict) -> dict:\n",
        "  video_model = replicate.models.get('cjwbw/damo-text-to-video')\n",
        "  video_version = video_model.versions.get(\"1e205ea73084bd17a0a3b43396e49ba0d6bc2e754e9283b2df49fad2dcf95755\")\n",
        "  descriptions = DoubleNewlineOutputParser().parse(inputs['video_descriptions'])\n",
        "  \n",
        "  predictions = []\n",
        "\n",
        "  for description in descriptions:\n",
        "      print(f\"Creating video prediction for '{description}'...\")\n",
        "      video_prediction = replicate.predictions.create(version=video_version, \n",
        "                                                      input={\"prompt\": description, \"num_frames\": 50, \"fps\": 10}) # 5s videos\n",
        "      predictions.append(video_prediction)\n",
        "  return {'video_predictions': predictions}\n",
        "\n",
        "video_predictions_chain = TransformChain(input_variables=['video_descriptions'], output_variables=['video_predictions'], transform=transform_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtPhTZwjEKHP"
      },
      "outputs": [],
      "source": [
        "# LLMChain to create the replicate predictions for our bark model\n",
        "def transform_func(inputs: dict) -> dict:\n",
        "  audio_model = replicate.models.get(\"suno-ai/bark\")\n",
        "  audio_version = audio_model.versions.get(\"b76242b40d67c76ab6742e987628a2a9ac019e11d56ab96c4e91ce03b79b2787\")\n",
        "  parsed_script = DoubleNewlineOutputParser().parse(inputs['script'])\n",
        "  \n",
        "  predictions = []\n",
        "\n",
        "  for line in parsed_script:\n",
        "      print(f\"Creating audio prediction for '{line}''...\")\n",
        "      audio_prediction = replicate.predictions.create(version=audio_version, \n",
        "                                                      input={\"prompt\": line, \"history_prompt\": \"announcer\"})\n",
        "      predictions.append(audio_prediction)\n",
        "  return {'audio_predictions': predictions}\n",
        "\n",
        "audio_predictions_chain = TransformChain(input_variables=['script'], output_variables=['audio_predictions'], transform=transform_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucZDP__PVl0C"
      },
      "outputs": [],
      "source": [
        "# LLMChain to create the cover image\n",
        "llm = OpenAI(temperature=.9)\n",
        "template = \"\"\"\n",
        "          Create a visual description for the following script. \n",
        "          \"{script}\"\"\n",
        "          \"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"script\"], template=template)\n",
        "text2image = Replicate(model=\"stability-ai/stable-diffusion:db21e45d3f7023abc2a46ee38a23973f6dce16bb082a930b0c49861f96d1e5bf\", \n",
        "                       input={'image_dimensions': '512x512', \"negative_prompt\": \"text, writing\"})\n",
        "title_image_chain = LLMChain(llm=text2image, prompt=prompt_template, output_key='title_image')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvZ-IHRoUpqb"
      },
      "outputs": [],
      "source": [
        "# LLMChain to write the thank you note at the end of our video\n",
        "template = \"\"\"Please come up with a creative and zany ending quote from our narrator. \n",
        "The script is what the narrator just read. We want to close things out.\n",
        "\n",
        "Make sure you add a \"And don't forget to like and subscribe!\" to the end of your output.\n",
        "\n",
        "You are a {narrator_adjectives} narrator.\n",
        "\n",
        "Script:\n",
        "{script}\n",
        "Ending quote: \n",
        "\"\"\"\n",
        "\n",
        "system_message_prompt = SystemMessage(content=\"You are a helpful assistant.\")\n",
        "human_message_prompt = HumanMessagePromptTemplate(prompt=PromptTemplate(\n",
        "                                                  template=template,\n",
        "                                                  input_variables=[\"script\", \"narrator_adjectives\"]))\n",
        "chat_prompt_template = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "ending_quote_chain = LLMChain(llm=chat, prompt=chat_prompt_template, output_key='ending_quote')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nTkKwQTY38m"
      },
      "outputs": [],
      "source": [
        "# LLMChain to create the prediction that generates the audio for the thank you note\n",
        "def transform_func(inputs: dict) -> dict:\n",
        "  audio_model = replicate.models.get(\"suno-ai/bark\")\n",
        "  audio_version = audio_model.versions.get(\"b76242b40d67c76ab6742e987628a2a9ac019e11d56ab96c4e91ce03b79b2787\")\n",
        "  ending_quote_prediction = replicate.predictions.create(version=audio_version, \n",
        "                                                      input={\"prompt\": inputs['ending_quote'], \"history_prompt\": \"announcer\"})\n",
        "  return {'ending_quote_prediction': ending_quote_prediction}\n",
        "\n",
        "ending_quote_prediction_chain = TransformChain(input_variables=['ending_quote'], output_variables=['ending_quote_prediction'], transform=transform_func)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkAkJofuhxe1"
      },
      "outputs": [],
      "source": [
        "# LLMChain to create the prediction that generates the audio for the title\n",
        "def transform_func(inputs: dict) -> dict:\n",
        "  audio_model = replicate.models.get(\"suno-ai/bark\")\n",
        "  audio_version = audio_model.versions.get(\"b76242b40d67c76ab6742e987628a2a9ac019e11d56ab96c4e91ce03b79b2787\")\n",
        "  title_prediction = replicate.predictions.create(version=audio_version, \n",
        "                                                      input={\"prompt\": inputs['title'], \"history_prompt\": \"announcer\"})\n",
        "  return {'title_audio_prediction': title_prediction}\n",
        "\n",
        "title_audio_prediction_chain = TransformChain(input_variables=['title'], output_variables=['title_audio_prediction'], transform=transform_func)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqmmk5AaNJjg"
      },
      "outputs": [],
      "source": [
        "# LLMChain to create the prediction that generates the background music\n",
        "def transform_func(inputs: dict) -> dict:\n",
        "  model = replicate.models.get(\"riffusion/riffusion\")\n",
        "  version = model.versions.get(\"8cf61ea6c56afd61d8f5b9ffd14d7c216c0a93844ce2d82ac1c9ecc9c7f24e05\")\n",
        "  music_prediction = replicate.predictions.create(version=version, input={\"prompt\": inputs['music_style']})\n",
        "\n",
        "  return {'music_prediction': music_prediction}\n",
        "\n",
        "music_prediction_chain = TransformChain(input_variables=['music_style'], output_variables=['music_prediction'], transform=transform_func)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynqnG0SR3nwW"
      },
      "source": [
        "# ‚ùî What should the video be about?\n",
        "> ‚¨áÔ∏è Here's the fun part. Every time we make a new video, we'll want to change the variables below and re-run from here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AooP0lrl3nf5"
      },
      "outputs": [],
      "source": [
        "TOPIC = \"how to walk through walls\" # make sure it has \"how to _\"\n",
        "NARRATOR_ADJECTIVES = \"shakespeare\"\n",
        "MUSIC_STYLE = \"spooky\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyQwZW-24M10"
      },
      "source": [
        "# üëü Run the Chain!\n",
        "Now, let's execute the chain we created. This is relatively fast, because the chains that create long-running predictions (like the video_predictions_chain) make asynchronous calls to the Replicate API.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5MHzwYUE95h",
        "outputId": "5181496a-6cec-4833-8c82-884ab840bb08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "Creating video prediction for 'Person dreaming of passing through walls in Verona.'...\n",
            "Creating video prediction for 'Determined person preparing for wall challenge.'...\n",
            "Creating video prediction for 'Person visualizing and building courage near wall.'...\n",
            "Creating video prediction for 'Individual approaching wall with confidence and power.'...\n",
            "Creating video prediction for 'Gently merging with wall as if embracing it.'...\n",
            "Creating video prediction for 'Person accepting outcome, embracing magical pursuit.'...\n",
            "Creating audio prediction for 'In fair Verona, where we lay our scene, passing through walls hath been a dream.''...\n",
            "Creating audio prediction for 'Fear thee not, as destined we unveil, thrice steps to conquer the rigid trail.''...\n",
            "Creating audio prediction for 'First, visualize vividly, the unyielding barrier beckons. Believe in thyself, and let courage strengthen.''...\n",
            "Creating audio prediction for 'Next, approach thee a wall, faithfully steady. Mind o'er matter, be powerful and ready.''...\n",
            "Creating audio prediction for 'On touching the barrier, a gentle pace. Merge with it, like a fervent embrace.''...\n",
            "Creating audio prediction for 'Success or defeat, matters not, my friend. For in pursuit of magic, we transcend.''...\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Run the chain\n",
        "overall_chain = SequentialChain(chains=[script_chain, \n",
        "                                        title_chain, \n",
        "                                        video_descriptions_chain, \n",
        "                                        video_predictions_chain, \n",
        "                                        audio_predictions_chain,\n",
        "                                        ending_quote_chain,\n",
        "                                        ending_quote_prediction_chain,\n",
        "                                        title_image_chain,\n",
        "                                        title_audio_prediction_chain,\n",
        "                                        music_prediction_chain\n",
        "                                        ], input_variables=['topic', 'narrator_adjectives', 'music_style'], output_variables=['script', 'video_descriptions', 'title', 'video_predictions', 'audio_predictions', 'ending_quote', 'title_image', 'ending_quote_prediction', 'title_audio_prediction', 'music_prediction'], verbose=True)\n",
        "chain_output = overall_chain({\"topic\": TOPIC, \"narrator_adjectives\": NARRATOR_ADJECTIVES, \"music_style\": MUSIC_STYLE})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_hvy--bLtl-"
      },
      "outputs": [],
      "source": [
        "# unpack outputs\n",
        "title = chain_output['title']\n",
        "script = chain_output['script']\n",
        "split_script = script.split('\\n\\n')\n",
        "video_descriptions = chain_output['video_descriptions'].split('\\n\\n')\n",
        "video_predictions = chain_output['video_predictions']\n",
        "audio_predictions = chain_output['audio_predictions']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5I0L55h4epL"
      },
      "source": [
        "> üö® This sanity check can sometimes fail. If it does, you only need to re-run the prior two blocks (Run the Chain! onwards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chcuZQGx6xjr"
      },
      "outputs": [],
      "source": [
        "# sanity check\n",
        "assert len(split_script) == 6\n",
        "assert len(video_descriptions) == 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Znto050hQCjM"
      },
      "source": [
        "# ‚è≥ Wait for our async predictions to complete\n",
        "Here's a helper to check in on our predictions. This usually takes a minute or two."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChI2MpOfQJWs"
      },
      "outputs": [],
      "source": [
        "def all_done(predictions):\n",
        "    return set([p.status for p in predictions]) == {'succeeded'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4PwZ_oZf9Ks"
      },
      "outputs": [],
      "source": [
        "all_predictions = chain_output['video_predictions'] + \\\n",
        "                  chain_output['audio_predictions'] + \\\n",
        "                  [chain_output['ending_quote_prediction']] + \\\n",
        "                  [chain_output['title_audio_prediction']] + \\\n",
        "                  [chain_output['music_prediction']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUkZmw0iQIGR",
        "outputId": "6a10e283-09fc-430b-c44e-f1fed4cb21b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions complete\n"
          ]
        }
      ],
      "source": [
        "done = False\n",
        "\n",
        "while not done:\n",
        "  [p.reload() for p in all_predictions]\n",
        "  for p in all_predictions:\n",
        "    print(f'https://replicate.com/p/{p.id}', p.status)\n",
        "  done = all_done(all_predictions)\n",
        "  time.sleep(2)\n",
        "  output.clear()\n",
        "\n",
        "print(\"Predictions complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkvtnprozZyQ"
      },
      "source": [
        "# ü™° Stitch them all together!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZnV3CMGP8Yc"
      },
      "outputs": [],
      "source": [
        "video_urls = [v.output for v in video_predictions]\n",
        "audio_urls = [a.output['audio_out'] for a in audio_predictions]\n",
        "music_url = chain_output['music_prediction'].output['audio']\n",
        "subtitles = split_script\n",
        "title_image_url = chain_output['title_image']\n",
        "title_audio_url = chain_output['title_audio_prediction'].output['audio_out']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUEDoBPlTHAd"
      },
      "source": [
        "Thank you to GPT-4 for this code. This scary looking block stitches together our video and narrator content, and adds background music."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6Nw3oggwTi9",
        "outputId": "5b1b5f17-56ee-41dd-aec7-63ebddb78e5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video how_to.mp4.\n",
            "MoviePy - Writing audio in how_toTEMP_MPY_wvf_snd.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Writing video how_to.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready how_to.mp4\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import os\n",
        "import moviepy.editor as mp\n",
        "import moviepy.video.fx.all as vfx\n",
        "import textwrap\n",
        "from moviepy.editor import *\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Download video and audio files\n",
        "video_files = []\n",
        "audio_files = []\n",
        "for i, url in enumerate(video_urls):\n",
        "    response = requests.get(url)\n",
        "    video_filename = f\"temp_video{i}.mp4\"\n",
        "    with open(video_filename, \"wb\") as video_file:\n",
        "        video_file.write(response.content)\n",
        "    video_files.append(video_filename)\n",
        "\n",
        "for i, url in enumerate(audio_urls):\n",
        "    response = requests.get(url)\n",
        "    audio_filename = f\"temp_audio{i}.mp3\"\n",
        "    with open(audio_filename, \"wb\") as audio_file:\n",
        "        audio_file.write(response.content)\n",
        "    audio_files.append(audio_filename)\n",
        "\n",
        "# Load and process video and audio files\n",
        "processed_videos = []\n",
        "for i, audio_file in enumerate(audio_files):\n",
        "    video = mp.VideoFileClip(video_files[i])\n",
        "    audio = mp.AudioFileClip(audio_file)\n",
        "\n",
        "    # Loop the video for the duration of the audio\n",
        "    looped_video = mp.concatenate_videoclips([video] * int(audio.duration // video.duration + 1))\n",
        "\n",
        "    # Set the audio of the video to the audio file\n",
        "    video_with_audio = looped_video.set_audio(audio)\n",
        "    processed_videos.append(video_with_audio)\n",
        "\n",
        "# Concatenate all the processed videos\n",
        "final_video = mp.concatenate_videoclips(processed_videos)\n",
        "\n",
        "## The following adds the title image / narration to the video.\n",
        "# Add this function to create the text image\n",
        "def txt_image(img, txt, font_size, color):\n",
        "    image = img.copy()\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    draw.text((50, 50), txt, fill=(255, 255, 0))  \n",
        "    # font = ImageFont.load_default().font_variant(size=font_size)\n",
        "    # draw.text((50, 50), txt, font=font, fill=color)\n",
        "    return image\n",
        "\n",
        "# Download and create the image clip\n",
        "image_url = title_image_url\n",
        "response = requests.get(image_url)\n",
        "img = Image.open(BytesIO(response.content))\n",
        "\n",
        "# Resize the image to match the video dimensions\n",
        "img_resized = img.resize((1200, 900))\n",
        "\n",
        "# Download the audio file\n",
        "audio_url = chain_output['ending_quote_prediction'].output['audio_out']\n",
        "response = requests.get(audio_url)\n",
        "with open(\"temp_audio_ending.mp3\", \"wb\") as audio_file:\n",
        "    audio_file.write(response.content)\n",
        "\n",
        "# Create the audio clip\n",
        "audio_ending = AudioFileClip(\"temp_audio_ending.mp3\")\n",
        "\n",
        "# make title empty for now, couldn't figure out how to get it bigger\n",
        "text = chain_output['title']\n",
        "img_text = ImageClip(np.asarray(txt_image(img_resized, txt=\"\", font_size=48, color=\"white\")), duration=4)\n",
        "\n",
        "# Set the audio of the image clip to the audio file and trim it to the same duration\n",
        "img_text_audio_ending = mp.concatenate_videoclips([img_text] * int(audio_ending.duration // img_text.duration + 1))\n",
        "img_text_audio_ending = img_text.set_audio(audio_ending)\n",
        "\n",
        "# Download the title page audio file\n",
        "audio_url = chain_output['title_audio_prediction'].output['audio_out']\n",
        "response = requests.get(audio_url)\n",
        "with open(\"temp_audio_title.mp3\", \"wb\") as audio_file:\n",
        "    audio_file.write(response.content)\n",
        "\n",
        "# Create the audio clip\n",
        "audio_beginning = AudioFileClip(\"temp_audio_title.mp3\")\n",
        "\n",
        "# Set the audio of the image clip to the audio file and trim it to the same duration\n",
        "img_text_audio_beginning = mp.concatenate_videoclips([img_text] * int(audio_beginning.duration // img_text.duration + 1))\n",
        "img_text_audio_beginning = img_text.set_audio(audio_beginning)\n",
        "\n",
        "# Concatenate the image clip with the processed videos\n",
        "width, height = processed_videos[0].size\n",
        "title_video = img_text_audio_beginning.resize((width, height))\n",
        "ending_video = img_text_audio_ending.resize((width, height))\n",
        "\n",
        "processed_videos.insert(0, title_video)\n",
        "processed_videos.append(ending_video)\n",
        "\n",
        "final_video = concatenate_videoclips(processed_videos)\n",
        "\n",
        "# Download the background audio file\n",
        "bg_audio_url = music_url\n",
        "response = requests.get(bg_audio_url)\n",
        "with open(\"temp_bg_audio.mp3\", \"wb\") as audio_file:\n",
        "    audio_file.write(response.content)\n",
        "\n",
        "# Create the background audio clip\n",
        "bg_audio = AudioFileClip(\"temp_bg_audio.mp3\")\n",
        "\n",
        "# Calculate the duration of the final video\n",
        "video_duration = final_video.duration\n",
        "\n",
        "# Loop the background audio to match the final video's duration\n",
        "bg_audio_looped = bg_audio.fx(afx.audio_loop, duration=video_duration)\n",
        "bg_audio_looped = bg_audio_looped.volumex(0.5)\n",
        "\n",
        "# Overlay the background audio with the audio from the final video\n",
        "final_audio = CompositeAudioClip([final_video.audio, bg_audio_looped])\n",
        "\n",
        "# Set the audio of the final video to the combined audio\n",
        "final_video_with_bg_audio = final_video.set_audio(final_audio)\n",
        "\n",
        "# Save the final video\n",
        "final_video_with_bg_audio.write_videofile(f\"how_to.mp4\", codec='libx264', audio_codec='aac')\n",
        "\n",
        "# Clean up temporary files\n",
        "for video_file, audio_file in zip(video_files, audio_files):\n",
        "    os.remove(video_file)\n",
        "for audio_file in audio_files:\n",
        "    os.remove(audio_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWwsGXaVhgwF"
      },
      "source": [
        "# üçø Watch the video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "V30X3XRxyo_V",
        "outputId": "f5942d92-9e84-4d9d-d5fb-bee49d8c6381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mThis cell output is too large and can only be displayed while logged in.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open(f'how_to.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "script"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "2AuFX9mlihfB",
        "outputId": "167226e7-0b5a-44fe-cbd6-084f0810d027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3833de1724ab>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'script' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or_M30Ck7Hxb"
      },
      "source": [
        "# üß† Things we learned:\n",
        "- It's hard to get the open source base LLMs like dolly/stable-lm to respond with structured text (like a consistent 3 paragraph response)\n",
        "- You can use SequentialChain in LangChain to handle multiple inputs and outputs\n",
        "- You can use a TransformChain to run arbitrary functions in your LangChain. This is how I'm running multiple async Replicate calls\n",
        "- Sometimes the damo/text-to-video output is fuzzy. Especially when the video description is vague. So it's really important that our LLM has a concrete description for what the video should be! It would be really cool if our LLM was multi-modal so we could feed back the video output and say \"if this is fuzzy can you make the description more concrete and try again?\"\n",
        "- GPT-4 was really helpful for stitching all the clips together! The final product is combining images, videos, narration clips, and background music (at 50% volume), and it all sort of works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kv7v8XL27KMw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}